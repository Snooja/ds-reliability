{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Data-Driven Reliability Metrics </center>\n",
    "## <center> Week 3 </center>\n",
    "## <center> Interactive Notebook </center>\n",
    "\n",
    "\n",
    "<center>üìö Source: W3-1 Data-Driven Reliability Metrics</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "‚ö†Ô∏è To make the most out of this notebook, some level of Python programing is desirable. If you've never used Python (or any other programing language) checkout resources [here](https://wiki.python.org/moin/BeginnersGuide/Programmers) to get up to speed. This is not mandatory, but will allow you to modify the code examples and complete the activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "‚ö†Ô∏è This notebook has interactive elements, if you have any issues running section `3.C` pleasure ensure that your notebook is set-up to use `ipywidgets` by running `jupyter nbextension enable --py widgetsnbextension --sys-prefix` in the jupyterhub terminal or reach out for help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Notebook Overview\n",
    "Within this notebook, we will move from toy to real failure datasets derived from maintenance work order records. The overarching application in this notebook is to semi-automatically calculate reliability metrics from maintenance work orders in a standardised, reproducible, low-resource, manner. To achieve this, first, we will gain insight into our data by performing exploratory data analysis (EDA). Second, we will become acquianted with fundamental natural language processing (NLP) techniques, that we will use to improve our datasets quality and for identification of end-of-life events. Third, we'll use expert logic to classify and reason about end-of-life events before performing statistical data life analysis via fitting to a 2-parameter Weibull distribution.\n",
    "\n",
    "<img src=\"./images/nb_3-1_excel_to_distribution.png\" alt=\"excel to reliability metrics\" width=\"75%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Legend\n",
    "- ‚ö° indicates a new concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Table of Contents\n",
    "* [Week 2 - Recap](#week-2-recap)\n",
    "* [3 - Overview of data-driven reliability metrics from maintenance work order data](#3-overview)\n",
    "* [3.A - Exploratory analysis of maintenance work order data](#3-A)\n",
    "    * [Activity 3.A](#3-A-activity)\n",
    "* [3.B - Preparing maintenance work order data for analysis](#3-B)\n",
    "    * [Activity 3.B](#3-B-activity)\n",
    "* [3.C - Identification of end-of-life events within unstructured maintenance text](#3-C)\n",
    "    * [3.C.1 - Basic key-word search](#3-C-1)\n",
    "        * [Activity 3.C.1 - Identifying end-of-life events (EOL) via key-word search](#3-C-1-activity)\n",
    "    * [3.C.2 - Expanded key-word search using word embeddings](#3-C-2)\n",
    "        * [Activity 3.C.2](#3-C-2-activity)\n",
    "* [3.D - Classification of identified end-of-life events](#3-D)\n",
    "    * [Activity 3.D](#3-D-activity)\n",
    "* [3.E - Reasoning about classified end-of-life events](#3-E)\n",
    "    * [Activity 3.E](#3-E-activity)\n",
    "* [3.F - Scalable statistical data life analysis](#3-F)\n",
    "* [Summary](#summary)\n",
    "* [Appendix](#appendix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Notebook Objectives\n",
    "- Perform exploratory data analysis on maintenance work order data\n",
    "- Load, wrangle and process maintenance work order data to extract MTBF in a standardised and reproducible way without external third-party software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning Outcomes\n",
    "- Understand how to load, wrangle and process maintenance work order data\n",
    "- Be comfortable with exploratory data analysis\n",
    "- Understand fundamental NLP concepts such as tokenization, ngrams, etc.\n",
    "- Understand how to visualise information in text using visualisation packages and dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap from Week 2 <a class=\"anchor\" id=\"week-2-recap\"></a>\n",
    "\n",
    "Provide your answers in [Menti](https://www.menti.com/yvgtwwv7er)\n",
    "- Do you have any questions or comments from last week?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install nb_black sklearn nltk pandas numpy pandas_profiling ipywidgets reliability plotly gensim regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load in the package [nb_black](https://github.com/dnanhkhoa/nb_black) to auto-format our code as we go. Formatting code is useful as it easier to collaborate and interpret what we write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Package for ensuring code we write is formatted nicely (see: https://github.com/dnanhkhoa/nb_black)\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Standard packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- [pandas](https://pandas.pydata.org/) - Package for data handling and wrangling\n",
    "- [pandas_profiling](https://github.com/ydataai/pandas-profiling) - Package for generating a profile over a pandas dataframe\n",
    "- [reliability](https://reliability.readthedocs.io/en/latest/index.html) - Package for performing Weibull analysis\n",
    "- [plotly](https://plotly.com/graphing-libraries/) - Package for interactive visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas_profiling import ProfileReport\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from reliability.Fitters import Fit_Weibull_2P\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "dir_path = os.path.abspath(\"\")\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3 - Overview of data-driven reliability metrics from maintenance work order data <a class=\"anchor\" id=\"3-overview\"></a>\n",
    "This section will explore identifying end-of-life events from the unstructured fields of maintenance work orders to allow us to classify them as failures, suspensions, or other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./images/nb_3-1_flow_diagram.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This section of the notebook will take us through different stages required to identify end-of-life events from real maintenance data, and using fortuitous data allow us to calculate reliability measures at scale. Illustrated in the figure, we will explore 6 mmain steps:\n",
    "\n",
    "- A: Maintenance work order records\n",
    "- B: **Data preparation**\n",
    "- C: **Identification** of end-of-life events\n",
    "- D: **Classification** of end-of-life events\n",
    "- E: **Reasoning** about end-of-life events, and\n",
    "- F: Statistical data life **analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.A - Maintenance work order records <a class=\"anchor\" id=\"3-A\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here we'll load a dataset containing ~40,000 maintenance records. Before we can use it in our notebook, we'll first ensure that the data is in the right format and all records have a short text description. If you have brought your own .csv dataset of work orders, feel free to place it into the `data` directory and replace the path in `data_path_large` to your files name. Please ensure that the column headers of your data are aligned with the `expected_cols` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# If you are using your own data, please change the name of the file to the name of your data.\n",
    "# Otherwise, use the URL link provided in this part of the program.\n",
    "data_path_large = \"<CORE_DATA_URL>\" # \"../data/<YOUR_CSV_FILE>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_cols = [\n",
    "    \"id\",\n",
    "    \"description\",\n",
    "    \"wo_order_type\",\n",
    "    \"total_actual_costs\",\n",
    "    \"actual_start_date\",\n",
    "    \"actual_finish_date\",\n",
    "    \"functional_loc_desc\",\n",
    "    \"functional_loc\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "date_cols = [\n",
    "    \"actual_start_date\",\n",
    "    \"actual_finish_date\",\n",
    "]\n",
    "\n",
    "data_path = (\n",
    "    data_path_large\n",
    "    if \"https\" in data_path_large\n",
    "    else os.path.join(dir_path, data_path_large)\n",
    ")\n",
    "\n",
    "df_large = pd.read_csv(\n",
    "    data_path,\n",
    "    parse_dates=date_cols,\n",
    "    dayfirst=True,\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    thousands=\",\",\n",
    "    dtype={\"description\": \"str\", \"total_actual_costs\": \"float\"},\n",
    ")\n",
    "\n",
    "assert set(expected_cols).issubset(\n",
    "    set(df_large.columns)\n",
    "), \"Uploaded data does not have all the expected columns\"\n",
    "\n",
    "# Lets remove any records that do not have a short text description\n",
    "df_large = df_large[~df_large[\"description\"].isna()]\n",
    "\n",
    "# Before we continue we'll convert the functional description into a more unique name for plotting later on\n",
    "df_large[\"object_desc\"] = df_large[\"functional_loc_desc\"].apply(\n",
    "    lambda desc: \" \".join(\n",
    "        [word for word in desc.replace(\"-\", \" \").split(\" \") if word.isalpha()]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lets review the dataset we have loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_large.head(2).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can quickly gain insight into our work order data by using the python package [pandas profiling](https://github.com/ydataai/pandas-profiling) which profiles the data giving us a quick overview of its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "profile = ProfileReport(\n",
    "    df_large,\n",
    "    title=\"Maintenance Work Order Data Profile\",\n",
    "    explorative=True,\n",
    "    progress_bar=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Further information about using pandas profiling can be found [here](https://pandas-profiling.ydata.ai/docs/master/). There are a lot of articles overviewing the capabilities of this exploratory data analysis tool such as [this one](https://towardsdatascience.com/learning-pandas-profiling-fc533336edc7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Activity 3.A <a class=\"anchor\" id=\"3-A-activity\"></a>\n",
    "\n",
    "Provide your answer in [Menti](https://www.menti.com/fb2iq6o8pe)\n",
    "\n",
    "- What proportion of work orders are PMs compared to corrective activities?\n",
    "- Find something interesting in the data to share with the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.B - Data preparation <a class=\"anchor\" id=\"3-B\"></a>\n",
    "<img src=\"./images/nb_3-1_flow_diagram-B.png\"/>\n",
    "</br>\n",
    "Now that we have familiarity with our dataset, we need to address the elephant in the room - data quality. Our dataset has many types of data classified as `date`, `categorical`, `numerical`, and `string`. In this section, we are particularly interested in the quality of the `string` data which manifests as the short text description of our work order records. Due to the unstructured nature of text in maintenance records, they can become unwieldly due to noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Noise in technical user-generated text is largely produced as a result of three factors:\n",
    "- 1. limited time,\n",
    "- 2. space constraints, and \n",
    "- 3. technical nature of the text\n",
    "\n",
    "As a result of these factors, technical texts such as maintenance work order short/long text will contains a lot of noise resulting from domain-specific terms, erroneous spelling, phonetic substitutions, informal grammar, etc.\n",
    "\n",
    "It should be noted that the considerations we will make, and processes we employ, are also directly applicable to other types of technical texts such as: condition monitoring reports (vibration analysis, etc.), notification text, down time accounting text, FLAC reports, maintenance procedures, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 3.B.1 - Fundamentals of natural language processing\n",
    "Before we dive into methods to improve the quality of our text data to support downstream tasks, first, lets familiarise ourself with some fundamental concepts of natural language processing (NLP) and get some some lingo/jargon along the way. There are a few more concepts also contained in the [Appendix]() for you to review at your leisure.\n",
    "\n",
    "If you're interested in learning more about NLP concepts in general CORE offers a [data science springboard](https://www.corehub.com.au/professional-program) that includes this and much more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For the following exercises we will use the texts contained within our maintenance record dataset. Moreover, we'll be using the Python [Natural Language Toolkit (NLTK)](https://www.nltk.org/) for this section of the notebook, however there are many packages available for processing, cleaning, and analysing text data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import required packages and functions from nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import ngrams\n",
    "from nltk.text import Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**‚ö° Concept - Corpus**</br>\n",
    "A corpus is a set of text documents, where a document can be a word, sentence, paragraph, report, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mwo_corpus = df_large[\"description\"].apply(lambda x: str(x)).tolist()\n",
    "print(f\"MWO corpus consists of {len(mwo_corpus)} texts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**‚ö° Concept - Tokenization**</br>\n",
    "Tokenization is the process of demarcating and possibly classifying sections of a string of input characters. The resulting tokens are then passed on to some other form of processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load the NLTK punkt tokenizer (find out more here - https://www.nltk.org/_modules/nltk/tokenize/punkt.html)\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Lets 'tokenize' a sentence using the NLTK (Tip: include punctuation (.,) and newlines (\\n) to see how it impacts tokenization)\n",
    "sentence = \"[CA] replace suction pipe on pump pu001 u/s\"\n",
    "# sentence = random.sample(mwo_corpus, 1)[0] # Uncomment to randomly sample data from the MWO corpus\n",
    "\n",
    "tokenized_sentence = word_tokenize(sentence)\n",
    "print(\n",
    "    f\"Input:\\n{sentence}\\nTokens:\\n{tokenized_sentence} ({len(tokenized_sentence)} tokens)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**‚ö° Concept - Vocabulary**</br>\n",
    "A vocabularly is set of terms that correspond to a particular subject matter. Now that we are familiar with tokenization, we can get a feel for how big our vocabulary is. The vocabulary of maintenance texts will be quite large, even though the length of the texts are small, as references to functional locations etc will be counted as unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Vocabulary depends on tokenization scheme, here we'll use an off-the-shelf tokenizer. Note we are removing all the casing from our\n",
    "# texts in this operation, as the MWO data uses capitalization inconsistently and erroneous. Hence, for all intensive-purposes, the computer\n",
    "# thinks that ON/on/On/oN are different tokens/words, although they mean the same thing.\n",
    "mwo_tokens = list(\n",
    "    itertools.chain.from_iterable([word_tokenize(text.lower()) for text in mwo_corpus])\n",
    ")\n",
    "mwo_vocab = set(mwo_tokens)\n",
    "\n",
    "# Lets get the vocab including casing for comparison.\n",
    "mwo_tokens_cased = list(\n",
    "    itertools.chain.from_iterable([word_tokenize(text) for text in mwo_corpus])\n",
    ")\n",
    "mwo_vocab_cased = set(mwo_tokens_cased)\n",
    "\n",
    "print(\n",
    "    f\"MWO corpus has a vocabulary size of {len(mwo_vocab)} (with casing: {len(mwo_vocab_cased)}, {(1 - len(mwo_vocab)/len(mwo_vocab_cased)) * 100:0.1f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the unique set of tokens (words) in our corpus (e.g. our vocabulary), it's also useful to know the word frequency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we'll use Pythons built in 'Counter' to count the tokens in our corpus.\n",
    "# This is a very handy native object for counting (see: https://docs.python.org/3/library/collections.html#collections.Counter)\n",
    "mwo_token_counts = Counter(mwo_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the some statistics of our corpus including the top 10 tokens and hapaxes. A hapaxe is a token with frequency of 1.\n",
    "print(f\"Total tokens:\\n{sum(mwo_token_counts.values())}\")\n",
    "print(f\"Top 10 tokens:\\n{mwo_token_counts.most_common(10)}\")\n",
    "print(f\"Rarest 10 tokens:\\n{mwo_token_counts.most_common()[-10:]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**‚ö° Concept - Stopwords**</br>\n",
    "Stopwords usually refer to the most common words in a language. For natural language processing applications, stopwords may contain less information than rarer words. For example in the text `replace the pump`, the token `the` is usually considered a stopword as it does not add any extra information (we can still understand the text as `replace pump`).\n",
    "\n",
    "For technical language like that found in maintenance work orders, many stopwords will not be used as these texts usually lack grammar. Moreover, standardised stopword lists include words that may be crucial for tasks such as identifying failure modes in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the NLTK stopwords\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Lets look at common stopwords (NLTK has these built in!)\n",
    "general_stopwords = stopwords.words(\"english\")\n",
    "print(general_stopwords)\n",
    "print(f\"\\nNumber of stopwords: {len(general_stopwords)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Lets see the impact of stop word removal from technical texts\n",
    "sentence_with_stopwords = \"replace pump not pumping to capacity\"\n",
    "# sentence_with_stopwords = random.sample(mwo_corpus, 1)[\n",
    "#     0\n",
    "# ]  # Uncomment to randomly sample data from the MWO corpus\n",
    "\n",
    "# Here we'll tokenize the sentence on whitespace and then remove any token that is within the stopword list\n",
    "# We lowercase the tokens when we check as the stopwords have no casing\n",
    "sentence_stopwords_rmv = \" \".join(\n",
    "    [\n",
    "        token\n",
    "        for token in sentence_with_stopwords.split(\" \")\n",
    "        if token.lower() not in general_stopwords\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Input:\\t{sentence_with_stopwords}\\nOutput:\\t{sentence_stopwords_rmv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**‚ö° Concept - N-grams**</br>\n",
    "Now that we are familiar with the concept of `tokens`, n-grams are simply the process of representing a set of n continguous (adjacent) tokens. Common names used under the concept of n-gram are unigram (n=1), bigram (n=2), trigram (n=3) where n is the number of contiguous tokens in the set. To make this more concrete, consider the tokens `[\"idler\", \"not\", \"working\"]`. Here, we can extract the following sets of n-grams:\n",
    "- unigram (n=1)\n",
    "    - token sets: `[\"idler\"]`, `[\"not\"]`, `[\"working\"]`\n",
    "    - grams: idler, not, working\n",
    "- bigram (n=2)\n",
    "    - token sets: `[\"idler\", \"not\"]`, `[\"not\", \"working\"]`\n",
    "    - grams: idler not, not working\n",
    "- trigram (n=3):\n",
    "    - token sets: `[\"idler\", \"not\", \"working\"]`\n",
    "    - grams: idler not working\n",
    "    \n",
    "As we can see, chunks of grams can represent different concepts, in this example the bigram \"not working\" has important meaning in maintenance applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Lets build some ngrams using NLTK (note: NLTK expects the sentence to be tokenized)\n",
    "sentence_for_ngrams = \"idler not working\"\n",
    "# sentence_for_ngrams = random.sample(mwo_corpus, 1)[0] # Uncomment to randomly sample data from the MWO corpus\n",
    "\n",
    "# Tokenize sentence (we'll use the NLTK punkt tokenizer here)\n",
    "sentence_for_ngrams_tokenized = word_tokenize(sentence_for_ngrams)\n",
    "\n",
    "n_values = [1, 2, 3, 4]\n",
    "\n",
    "for n in n_values:\n",
    "    # The ngram function returns a `zip` object\n",
    "    grams = [\" \".join(gram) for gram in ngrams(sentence_for_ngrams_tokenized, n)]\n",
    "    print(f\"{n}-grams: {grams}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "‚ö° **Concept - Phrases/Chunking**</br>\n",
    "As seen above, tokens within a text can form many different sized ngrams ('not', 'working' vs 'not working'). How do we know which ngrams should be formed between which tokens? To do this we can \"chunk\" or \"phrase\" our text. Here we will train a phrasing algorithm that will learn to detect common phrases aka multi-word expressions automatically from our corpus.\n",
    "\n",
    "This is an important tool in our NLP toolkit as it enables us to treat words as chunks of meaning rather than just a bag of individual words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ENGLISH_CONNECTOR_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Here we'll build a phrase model to detect common phrases\n",
    "# Find out more: https://radimrehurek.com/gensim/models/phrases.html\n",
    "\n",
    "# We need to feed in our corpus after it has been tokenized\n",
    "tokenized_mwo_corpus_for_embedding = [\n",
    "    word_tokenize(text.lower()) for text in mwo_corpus\n",
    "]\n",
    "\n",
    "# Now we'll train our phrasing model but only account for words with a minimum frequency of 5\n",
    "# (you can make this lower such as 1 to account for hapaxes)\n",
    "phrase_model = Phrases(\n",
    "    tokenized_mwo_corpus_for_embedding,\n",
    "    min_count=5,\n",
    "    threshold=10,\n",
    "    connector_words=ENGLISH_CONNECTOR_WORDS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Lets try the phrasing model out\n",
    "text_to_phrase = \"change out pump impeller - not working\"\n",
    "text_to_phrase_tokenized = word_tokenize(text_to_phrase)\n",
    "\n",
    "# Now we'll pass it into our phrasing model\n",
    "phrase_model[text_to_phrase_tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**‚ö° Concept - Word Representation and Similarities**</br>\n",
    "The last concept we'll review is that of similarity between words. To make this concrete, lets think about the words `oil`, `fuel`, `coolant` and `water`. From our background knowledge, we know these share similar meaning, i.e. they are all fluids and they may be found in industrial contexts. However, how do we make machines gain this intution? One answer is `word embeddings`.\n",
    "\n",
    "Like the saying \"you can tell a word by the company that it keeps\", word embeddings are numerical representations that allows word with similar meaning to have similar values. Given a large enough corpus, these word embeddings can be learnt. \n",
    "\n",
    "All state-of-the-art NLP use embeddings in one form or another, so an intuition toward them is important. However, diving into the details of embeddings is out of the scope of this notebook, but if you're interested check out [here](https://machinelearningmastery.com/what-are-word-embeddings/) for further information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./images/nb_3-1_embeddings.png\" alt=\"word embeddings\" width=\"75%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here we'll load more packages\n",
    "- [gensim](https://radimrehurek.com/gensim/) - package used to learn word embeddings from text\n",
    "- [sklearn](https://scikit-learn.org/) - package for machine learning that we'll use for dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class EmbeddingTrainer:\n",
    "    def __init__(self):\n",
    "        self.min_token_count = 2\n",
    "        self.window_size = 3\n",
    "        self.model_size = 300\n",
    "\n",
    "    def train_model(self, docs, iterations: int = 50):\n",
    "        w2v_model = Word2Vec(\n",
    "            sentences=docs,\n",
    "            vector_size=self.model_size,\n",
    "            min_count=self.min_token_count,\n",
    "            window=self.window_size,\n",
    "            epochs=iterations,\n",
    "        )\n",
    "        print(f\"Model summary:\\n {w2v_model}\")\n",
    "        return w2v_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here we'll train a word embedding model called Word2Vec ([find out more here](https://radimrehurek.com/gensim/models/word2vec.html)). Please note that we will be doing this on the original corpus without any normalisation or cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# First lets tokenize our corpus using the punkt tokenizer from NLTK; we'll also lower case everything to\n",
    "# improve the support of each word. However, this is not prescriptive.\n",
    "tokenized_mwo_corpus_for_embedding = [\n",
    "    word_tokenize(text.lower()) for text in mwo_corpus\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Again, we'll build a phrase model to detect common phrases\n",
    "phrase_model = Phrases(\n",
    "    tokenized_mwo_corpus_for_embedding,\n",
    "    min_count=5,\n",
    "    threshold=10,\n",
    "    connector_words=ENGLISH_CONNECTOR_WORDS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Lets learn word embeddings from the text in our corpus\n",
    "# Here we will pass our tokenized corpus into our pretrained phrasing model\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=[phrase_model[t] for t in tokenized_mwo_corpus_for_embedding],\n",
    "    vector_size=300,\n",
    "    min_count=5,\n",
    "    window=3,\n",
    "    epochs=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Vocabulary size of the model we created\n",
    "print(f\"Size of word embedding vocabulary: {len(w2v_model.wv.index_to_key)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now that we have a numerical representation of words in our corpus, lets inspect some word similarities. Try words such as `seized`, `replaced`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def get_similar_words(model=w2v_model, word: str = \"\"):\n",
    "    try:\n",
    "        if word in w2v_model.wv.index_to_key:\n",
    "            sim_words = model.wv.most_similar(word)\n",
    "            print(\n",
    "                \"\\n\".join(\n",
    "                    [\n",
    "                        f\"{word} ({similarity*100:0.1f}%)\"\n",
    "                        for word, similarity in sim_words\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            print(\"Word not in vocabulary - try again!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed due to {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Caution: We need to ensure that we use words that exist in our vocabulary!\n",
    "base_word = \"seized\"\n",
    "\n",
    "get_similar_words(word=base_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è An important caveat for using embeddings in technical languages is the type of embedding may be restricted on what can be represented depending on the way the embeddings were learnt. In this notebook, we are using `word` embeddings, hence the embeddings are built off of the words in our vocabulary (over the minimum threshold we specified). Hence, the embedding model will NOT represent words that are hapaxes if we set our threshold to greater than 1, it will be considered OUT OF VOCABULARY.\n",
    "\n",
    "This is an important area for performing NLP on technical languages when using off-the-shelf embedding models (which is the default approach in NLP for most domains). First, important domain specific words may not be present (e.g. `primary_scraper` is not a common word in the English language and has specific meaning in technical contexts). Second, even if the embedding model can take the word, it doesn't mean that the context it creates from it, is correct.\n",
    "\n",
    "This isn't to say that off-the-shelf embeddings cannot be used, as they still have great utility (being trained on billions of words), they just need to be used with an understanding of their potential limitations in technical settings.\n",
    "\n",
    "Note: Not all embeddings are WORD embeddings, there are also sub-word embeddings, character embeddings, document embeddings, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets quickly quantify this issue that we've discussed above by comparing our domain-specific embeddings to those from a general domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets quickly look at this in action for word embeddings. Note - this might take a minute to download.\n",
    "# Source: https://radimrehurek.com/gensim/auto_examples/howtos/run_downloader_api.html#how-to-download-pre-trained-models-and-corpora\n",
    "corpus = api.load(\"text8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train embedding on general text corpus\n",
    "general_embedding_model = Word2Vec(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get similar words to our base word using the general model\n",
    "get_similar_words(model=general_embedding_model, word=base_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try a domain-specific abbreviation, 'c/o' and see what happens.\n",
    "get_similar_words(model=general_embedding_model, word=\"c/o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how similar words are\n",
    "word_a = \"seized\"\n",
    "word_b = \"failed\"\n",
    "print(f\"Similarity between {word_a} and {word_b}\")\n",
    "print(f\"Domain-specific model: {w2v_model.wv.similarity(word_a, word_b) * 100:0.1f}%\")\n",
    "print(\n",
    "    f\"General model: {general_embedding_model.wv.similarity(word_a, word_b) * 100:0.1f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Before we move onto the next section, lets visualise the embeddings we have learnt from our MWOs using the package `plotly`. Recall that the embeddings we created are of the size 300, hence, each word has 300 numerical values. To make this interpretable, we need to reduce the dimensionality to 2 so we can visualise it on a plot. To do this, we'll use a dimensionality reduction technique called [principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) (note there are many other techniques out there such as T-SNE, UMAP, autoencoders, etc.).\n",
    "\n",
    "The main reason for visualising embeddings is to identify semantic and syntactic trends in the data. For instance, words like 'replace', 'replacement', 'change out', 'repair' lie close together due to their semantics (they are activities), and 'frame' and 'frames' are close together due to syntax (plurality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# First we need to build a vector of words and their embeddings\n",
    "X = np.array([w2v_model.wv[word] for word in w2v_model.wv.index_to_key])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Lets reduce the dimensionality of our embedding array\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the representations learnt for individual words from our MWO corpus\n",
    "token_to_show = \"bearing\"\n",
    "token_to_show_sim_tokens = [\n",
    "    token_sim[0] for token_sim in w2v_model.wv.most_similar(token_to_show, topn=100)\n",
    "]\n",
    "\n",
    "df_token_sims = pd.DataFrame(\n",
    "    {\n",
    "        \"x\": X_pca[:, 0],\n",
    "        \"y\": X_pca[:, 1],\n",
    "        \"word\": w2v_model.wv.index_to_key,\n",
    "        \"label\": [\n",
    "            \"neighbour\" if token != token_to_show else \"token\"\n",
    "            for token in w2v_model.wv.index_to_key\n",
    "        ],\n",
    "        \"size\": [\n",
    "            1 if token != token_to_show else 10 for token in w2v_model.wv.index_to_key\n",
    "        ],\n",
    "    },\n",
    "    columns=[\"x\", \"y\", \"word\", \"label\", \"size\"],\n",
    ")\n",
    "\n",
    "# Filter for neighbouring similar tokens\n",
    "df_token_sims = df_token_sims[\n",
    "    df_token_sims[\"word\"].isin([token_to_show] + token_to_show_sim_tokens)\n",
    "]\n",
    "\n",
    "fig = px.scatter(\n",
    "    df_token_sims,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    text=\"word\",\n",
    "    title=f\"Maintenance Work Order PCA - Token: {token_to_show}\",\n",
    "    color=\"label\",\n",
    "    size=\"size\",\n",
    ")\n",
    "fig.update_traces(textposition=\"top center\")\n",
    "fig.update_layout(xaxis_title=\"Dimension 1\", yaxis_title=\"Dimension 2\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sense check: Lets look at the work orders with 'rtd' and 'bearing'\n",
    "[text for text in mwo_corpus if (\"rtd\" in text.lower()) & (\"bearing\" in text.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment the following code to visualise the entire embedding space\n",
    "## Visualise the 2-dimensional data and explore\n",
    "## Note: the data will become less 'messy' if you were to subset the dataset on particular assets of interest\n",
    "# tokens_to_show = 500\n",
    "# df_embed_original = pd.DataFrame(\n",
    "#     {\n",
    "#         \"x\": X_pca[:, 0],\n",
    "#         \"y\": X_pca[:, 1],\n",
    "#         \"word\": w2v_model.wv.index_to_key,\n",
    "#         \"label\": np.array([\"original\"] * len(X_pca)),\n",
    "#     },\n",
    "#     columns=[\"x\", \"y\", \"word\", \"label\"],\n",
    "# )\n",
    "\n",
    "# fig = px.scatter(\n",
    "#     df_embed_original.iloc[:tokens_to_show],\n",
    "#     x=\"x\",\n",
    "#     y=\"y\",\n",
    "#     text=\"word\",\n",
    "#     title=\"Principle Component Analysis - Embedded Words from maintenance work orders\",\n",
    "# )\n",
    "# fig.update_traces(textposition=\"top center\")\n",
    "# fig.update_layout(xaxis_title=\"Dimension 1\", yaxis_title=\"Dimension 2\")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being able to visualise our technical text can provide us with immediate insight into the way information is stated in text, but what else can we do with these numerical representations? We could:\n",
    "- Find activities performed in response to an end-of-life event of interest\n",
    "- Aggregate the word level embeddings for entire work orders and have a document embedding we could use to find work orders that share similar meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activity 3.B.1\n",
    "\n",
    "Post your answers on [Menti](https://www.menti.com/84a7o17fvr)\n",
    "- What are your thoughts on the fundamentals of NLP? Is there anything surprising that you have learnt?\n",
    "- How do you deal with noisy text in your work?\n",
    "- How would you tackle fixing this text - \"re*pl;ace the ## 1 p/p   impeller!! @ the pump stn & clean/inspect o-rings\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.B.1 - Takeaways\n",
    "- Tokenization is a crucial part of natural language processing\n",
    "- The cleanliness of data can impact on the ability to tokenize\n",
    "- Using text preprocessing blindly can accidentally remove meaning from texts e.g. removing stopwords that have technical important\n",
    "- Embeddings are a powerful technique for numerically representing meaning in text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 3.B.2 - Text Cleaning\n",
    "There are various methods that can be adopted to improve the quality of text data such as using off-the-shelf tools, dictionaries, learning algorithms, etc.\n",
    "\n",
    "For technical language, which maintenance records are composed of, off-the-shelf tools do not work very effectively due to the specific language used. Dictionaries are a popular approach due to their simplicity, for example a dictionary could be of the format:\n",
    "\n",
    "```\n",
    "rp : replace\n",
    "rple : replace\n",
    "replc : replace\n",
    "```\n",
    "\n",
    "However, the use of such dictionaries is challenging as they are most useful in `find-and-replace` strategies. For example, `rple pump impeller` we could find that the word `rple` exists in our dictionary and normalise it to `replace`. This would result in improved coverage of any techniques we were to apply to our data, however in instances where words are ambiguous and change meaning based on their context, this becomes difficult. Consider the two texts `replace a/c cable` and `replace a/c vents`. The former may be an abbreviation for `a/c -> alternating current` and the latter `a/c -> air conditioner`, hence, by doing a `find-and-replace` strategy we may erroneously normalise our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here we'll develop a function to perform some basic text cleaning. In this notebook, our main focus is on identifying words and phrases that indicate end-of-life, hence identifiers, numbes, etc, can be removed as they increase our vocabulary unncessarily. The steps we'll perform here, including:\n",
    "- Removing special characters\n",
    "    - `**c/out gearbox` ‚Üí `c/out gearbox`\n",
    "- Removing superfluous whitespace\n",
    "    - `idler    bearing replacement` ‚Üí `idler bearing replacement`\n",
    "- Removing stopwords (cautiously)\n",
    "    - `replace the bearing` ‚Üí `replace bearing`\n",
    "- Replacing known noisy words/abbreviations with a controlled dictionary\n",
    "    - `c/out bearing` ‚Üí `change out bearing`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lets write functions to perform each of these cleaning stages. We'll use an exemplar text that has the form of `re*pl;ace the ## 1 p/p   impeller!! @ the pump stn & clean/inspect o-rings` and our goal is to normalise it deterministacally to **`replace the number 1 pump impeller at the pump station and clean / inspect o-rings`**.\n",
    "\n",
    "Note: What we're creating here are called [lambda functions](https://www.w3schools.com/python/python_lambda.asp) in python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here we'll use the a package called `regex` which is for [regular expressions](https://en.wikipedia.org/wiki/Regular_expression) in Python. Note there is a native package in python called `re` but it can be limited in its flexibility. There are many tools to help you develop regular expressions online such as [regexr](https://regexr.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_text = \"re*pl;ace the ## 1 p/p   impeller!! @ the pump stn & clean/inspect o-rings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Remove all characters except for alphanumerical and reserved special characters e.g. @, -, #, /, and .\n",
    "# Note: We're using regular expressions here, so some characters need to be 'escaped' as they are special 'metacharacters'\n",
    "# Refer to this for further information: https://www3.ntu.edu.sg/home/ehchua/programming/howto/Regexe.html\n",
    "\n",
    "chars_to_keep = \"&\\.\\#\\@\\/-\"\n",
    "\n",
    "fnc_rmv_chars = lambda text: regex.sub(rf\"[^a-zA-Z0-9 {chars_to_keep}]\", \"\", text)\n",
    "fnc_rmv_chars(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Remove duplicate reserved special characters (like ##, @@@, etc.)\n",
    "# This way if we want to normalise # to number and we have ## we wont' get numbernumber\n",
    "\n",
    "fnc_rmv_dupe_chars = lambda text: regex.sub(rf\"([{chars_to_keep}])\\1+\", r\"\\1\", text)\n",
    "\n",
    "fnc_rmv_dupe_chars(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break any erroneous hyphenated or compound abbreviations\n",
    "# We want to keep things like 'o-ring' and 'c/o' but fix things like 'CV001-replace' ‚Üí 'CV001 - replace'\n",
    "# and 'change-out' ‚Üí 'change out', 'Plate/Lower' ‚Üí 'plate / lower', 'inspect/replace' ‚Üí 'inspect / replace'\n",
    "\n",
    "test_text_2 = \"CVR231-Replace brake switch and change/out o-ring\"\n",
    "\n",
    "fnc_fix_compound_hyphen = lambda text: regex.sub(r\"(?<=\\w{3,})-(?=\\w{3,})\", \" - \", text)\n",
    "fnc_fix_compound_fwd_slash = lambda text: regex.sub(\n",
    "    r\"(?<=\\w{3,})\\/(?=\\w{3,})\", \" / \", text\n",
    ")\n",
    "fnc_fix_compound_chars = lambda text: fnc_fix_compound_fwd_slash(\n",
    "    fnc_fix_compound_hyphen(text)\n",
    ")\n",
    "\n",
    "fnc_fix_compound_chars(test_text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Removing superfluous whitespace (e.g. more than 1 whitespace between words)\n",
    "# Doing this means we do not have tokens that are whitespace e.g. 'replace   engine' ‚Üí ['replace', '', '', 'engine']\n",
    "# We also 'strip' leading and ending whitespace e.g. ' replace engine ' ‚Üí 'replace engine'\n",
    "\n",
    "fnc_rmv_whitespace = lambda text: regex.sub(r\" {2,}\", \" \", text).strip()\n",
    "fnc_rmv_whitespace(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Removing stopwords (cautiously)\n",
    "# Note: stopwords are unigrams, so we can simply split our text on whitespace and check whether each token\n",
    "# is in a stopword list; if it is, skip it, otherwise keep it.\n",
    "\n",
    "# Recall that some stopwords are meaningful for us, hence we will filter the list down first.\n",
    "\n",
    "stopwords_to_keep = [\n",
    "    \"on\",\n",
    "    \"off\",\n",
    "    \"over\",\n",
    "    \"under\",\n",
    "    \"no\",\n",
    "    \"not\",\n",
    "    \"don\",\n",
    "    \"don't\",\n",
    "    \"aren\",\n",
    "    \"aren't\",\n",
    "    \"no\",\n",
    "    \"not\",\n",
    "    \"didn't\",\n",
    "    \"doesn\",\n",
    "    \"doesn't\",\n",
    "    \"hadn\",\n",
    "    \"hadn't\",\n",
    "    \"hasn\",\n",
    "    \"hasn't\",\n",
    "    \"haven\",\n",
    "    \"haven't\",\n",
    "    \"isn\",\n",
    "    \"isn't\",\n",
    "    \"won\",\n",
    "    \"won't\",\n",
    "    \"wouldn\",\n",
    "    \"wouldn't\",\n",
    "]\n",
    "filtered_stopwords = [\n",
    "    word for word in stopwords.words(\"english\") if word not in stopwords_to_keep\n",
    "]\n",
    "\n",
    "fnc_rmv_stopwords = lambda text: \" \".join(\n",
    "    [token for token in text.split(\" \") if token not in filtered_stopwords]\n",
    ")\n",
    "\n",
    "fnc_rmv_stopwords(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Normalising noisy words with a controlled dictionary\n",
    "\n",
    "word_normalisation_dictionary = {\n",
    "    \"#\": \"number\",\n",
    "    \"@\": \"at\",\n",
    "    \"u/s\": \"unserviceable\",\n",
    "    \"changeout\": \"change out\",\n",
    "    \"c/o\": \"change out\",\n",
    "    \"c/out\": \"change out\",\n",
    "    \"rplc\": \"replace\",\n",
    "    \"p/p\": \"pump\",\n",
    "    \"stn\": \"station\",\n",
    "    \"repl\": \"replace\",\n",
    "    \"&\": \"and\",\n",
    "    \"rpl\": \"replace\",\n",
    "}\n",
    "\n",
    "# Note this could be performed entirely with more complex regular expressions\n",
    "def dictionary_normalisation(text: str, norm_dict: dict) -> str:\n",
    "    for noisy_word, clean_word in norm_dict.items():\n",
    "        if noisy_word in chars_to_keep:\n",
    "            text = text.replace(noisy_word, f\" {clean_word} \")\n",
    "        text = regex.sub(rf\"(?<!-)\\b{noisy_word}\\b(?!-)\", f\" {clean_word} \", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# test on test_text\n",
    "dictionary_normalisation(text=test_text, norm_dict=word_normalisation_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Putting them all together. Note the order of these operations is important.\n",
    "# We don't want to try and remove stopwords or normalise words when they have special characters, or extra whitespace etc.\n",
    "\n",
    "\n",
    "def text_cleaner(text: str, norm_dict: dict) -> str:\n",
    "    \"\"\"Cleans and normalises a given text. Steps performed include: remove casing,\n",
    "    removing special characters, removing duplicate non-alphanumerical characters, fix erroneous compound chars,\n",
    "    removing stopwords, normalising terms using a dictionary, and remvoing superfluous whitespace\"\"\"\n",
    "    text = text.lower()\n",
    "    text = fnc_rmv_chars(text)\n",
    "    text = fnc_rmv_dupe_chars(text)\n",
    "    text = fnc_fix_compound_chars(text)\n",
    "    text = fnc_rmv_stopwords(text)\n",
    "    text = dictionary_normalisation(text, norm_dict)\n",
    "    text = fnc_rmv_whitespace(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Lets check whether we have achieved our goal of normalising the starting text\n",
    "cleaned_text = text_cleaner(text=test_text, norm_dict=word_normalisation_dictionary)\n",
    "print(f\"Input: {test_text}\\nOutput: {cleaned_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets quickly look at the impact of our cleaning on a set of corrective work orders\n",
    "corrective_texts_to_clean = (\n",
    "    df_large[df_large[\"wo_order_type\"] == \"PM01\"][\"description\"].sample(n=10).tolist()\n",
    ")\n",
    "\n",
    "corrective_text_pairs = [\n",
    "    (\n",
    "        original_text,\n",
    "        text_cleaner(text=original_text, norm_dict=word_normalisation_dictionary),\n",
    "    )\n",
    "    for original_text in corrective_texts_to_clean\n",
    "]\n",
    "\n",
    "for pair in corrective_text_pairs:\n",
    "    original_text, cleaned_text = pair\n",
    "    print(f\"{original_text} ‚Üí {cleaned_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps we've seen are typical for preprocessing noisy natural language texts, however newer technology has currently started to focus on this problem using deep learning algorithms. Similar to technologies such as the spell checker on your mobile phone, and services such as Grammarly, these technologies aim to fix noisy text without writing heuristics as we've done here.\n",
    "\n",
    "However, the technical text which you deal with is not amenable to these types of services as the language used is too domain-specific. We will not go into the details of it in this notebook, but research in the [UWA NLP-TLP group](https://nlp-tlp.org/) is developing translation models to go from noisy technical text to clean technical text without any rules. Just like English to French translation, the group treats noisy to clean text in a similar way, going from:\n",
    "\n",
    "```\n",
    "Input: re*pl;ace the ## 1 p/p   impeller!! @ the pump stn\n",
    "Output: replace number 1 pump impeller at pump station\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move onto the next section, lets quickly overlay our data after its cleaned on the embeddings we made previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dimensionality reduced embedding data for cleaned dataset\n",
    "cleaned_tokenized_mwo_corpus_for_embedding = [\n",
    "    word_tokenize(text_cleaner(text=text, norm_dict=word_normalisation_dictionary))\n",
    "    for text in mwo_corpus\n",
    "]\n",
    "cleaned_phrase_model = Phrases(\n",
    "    cleaned_tokenized_mwo_corpus_for_embedding,\n",
    "    min_count=5,\n",
    "    threshold=10,\n",
    "    connector_words=ENGLISH_CONNECTOR_WORDS,\n",
    ")\n",
    "cleaned_w2v_model = Word2Vec(\n",
    "    sentences=[\n",
    "        cleaned_phrase_model[t] for t in cleaned_tokenized_mwo_corpus_for_embedding\n",
    "    ],\n",
    "    vector_size=300,\n",
    "    min_count=5,\n",
    "    window=3,\n",
    "    epochs=50,\n",
    ")\n",
    "X_cleaned = np.array(\n",
    "    [cleaned_w2v_model.wv[word] for word in cleaned_w2v_model.wv.index_to_key]\n",
    ")\n",
    "pca_cleaned = PCA(n_components=2)\n",
    "X_pca_cleaned = pca.fit(X_cleaned).transform(X_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_comparison(\n",
    "    model_a, model_b, X_pca_a, X_pca_b, token_to_show: str, topn: int = 100\n",
    "):\n",
    "    \"\"\"\n",
    "    Compares the dimensionality reduced representation of embedding models on a given token.\n",
    "\n",
    "    a denotes baseline model\n",
    "    b denotes cleaned model\n",
    "    \"\"\"\n",
    "\n",
    "    dfs = []\n",
    "    for name, model, X_pca in [(\"\", model_a, X_pca_a), (\"-cleaned\", model_b, X_pca_b)]:\n",
    "        token_to_show_sim_tokens = [\n",
    "            token_sim[0]\n",
    "            for token_sim in model.wv.most_similar(token_to_show, topn=topn)\n",
    "        ]\n",
    "\n",
    "        df_token_sims = pd.DataFrame(\n",
    "            {\n",
    "                \"x\": X_pca[:, 0],\n",
    "                \"y\": X_pca[:, 1],\n",
    "                \"word\": model.wv.index_to_key,\n",
    "                \"label\": [\n",
    "                    f\"neighbour{name}\" if token != token_to_show else f\"token{name}\"\n",
    "                    for token in model.wv.index_to_key\n",
    "                ],\n",
    "                \"size\": [\n",
    "                    1 if token != token_to_show else 10\n",
    "                    for token in model.wv.index_to_key\n",
    "                ],\n",
    "            },\n",
    "            columns=[\"x\", \"y\", \"word\", \"label\", \"size\"],\n",
    "        )\n",
    "\n",
    "        # Filter for neighbouring similar tokens\n",
    "        df_token_sims = df_token_sims[\n",
    "            df_token_sims[\"word\"].isin([token_to_show] + token_to_show_sim_tokens)\n",
    "        ]\n",
    "\n",
    "        dfs.append(df_token_sims)\n",
    "\n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the representations learnt for individual words from our MWO corpus\n",
    "comparison_token_to_show = \"bearing\"\n",
    "\n",
    "df_embed_comparison = embedding_comparison(\n",
    "    model_a=w2v_model,\n",
    "    model_b=cleaned_w2v_model,\n",
    "    X_pca_a=X_pca,\n",
    "    X_pca_b=X_pca_cleaned,\n",
    "    token_to_show=comparison_token_to_show,\n",
    "    topn=25,\n",
    ")\n",
    "\n",
    "fig = px.scatter(\n",
    "    df_embed_comparison,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    text=\"word\",\n",
    "    title=f\"Maintenance Work Order PCA - Token: {comparison_token_to_show}\",\n",
    "    color=\"label\",\n",
    "    size=\"size\",\n",
    ")\n",
    "fig.update_traces(textposition=\"top center\")\n",
    "fig.update_layout(xaxis_title=\"Dimension 1\", yaxis_title=\"Dimension 2\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because embedings are learnt based on co-occurrence, e.g. similiar words are used in similar contexts, we see that the cleaning effort makes a difference on embedding similarities by improving the numerical representations learnt. In the next sections of this notebook, and next week, we'll further see why this stage is important for natural language processing on technical texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A natural question that occurs when using dictionary based lexical normalisation is where do the dictionaries come from? There are two common methods - 1. you can curate them manually from your own knowledge and available resources, or 2. use available software such as [LexiClean](https://lexiclean.nlp-tlp.org) (developed by the UWA NLP-TLP group) which helps you quickly normalise texts and build replacement dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Activity 3.B.2 Part A <a class=\"anchor\" id=\"3-B-activity\"></a>\n",
    "\n",
    "Given the text below:\n",
    "- Tokenize it using the NLTK tokenizer, and\n",
    "- Extract all of the phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activity 3.B.2 Part A\n",
    "a3b2a_text = \"[PM] change out idler bearing.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the following cell to see the answer to this activity (after you try it!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./solutions/W3/tokenization_phrasing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3.B.2 Part B <a class=\"anchor\" id=\"3-B-2-activity\"></a>\n",
    "\n",
    "Given the following MWO corpus, first clean the texts and then find the total number of tokens and the top 10 most frequent tokens.\n",
    "\n",
    "Optional:\n",
    "- Top 10 rarest tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Corpus for getting intuition for fundemantals of NLP\n",
    "# this will include noisy data to ensure that we can highlight the rarity of words, etc.\n",
    "random.seed(1234)\n",
    "a3b2b_corpus = random.sample(mwo_corpus, 25)\n",
    "print(\"\\n\".join(a3b2b_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this area for activity 3.B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the following cell to see the answer to this activity (after you try it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./solutions/W3/corpus_stats.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.C - Identification of end-of-life events  <a class=\"anchor\" id=\"3-C\"></a>\n",
    "</br>\n",
    "<center>\n",
    "    <img src=\"./images/nb_3-1_flow_diagram-C.png\"/>\n",
    "</center>\n",
    "</br>\n",
    "Now that we have an intuition towards the fundamental concepts of natural language processing such that we can speak a common language, lets use what we've learnt to identify end-of-life events in maintenance work order short text. We treat end-of-life (EOL) events as any event that results in the inability of an item to function (e.g. functional failure). For example, \"mechanical seal blown on pump\" would indicate an EOL event for the pump.\n",
    "\n",
    "\n",
    "There are numerous ways this can be tackled, but the two we will explore in this notebook include:\n",
    "- 1. Basic keyword search (e.g. what we would do if we were using spreadsheet software), annd\n",
    "- 2. Expanded keyword search by using word embeddings to identify trigger terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For each of the proposed techniques that we will review, they become increasingly data intensive but more effective.\n",
    "- Basic keyword search - no annotated data, least effective\n",
    "- Expanded keyword search using embeddings - low amount of annotated data, more effective than basic keyword search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**‚ö° Concept - Annotation**</br>\n",
    "Annotation is the process of labeling data to guide models and teach them to predict the outcome we want. Consider the text `replace air compressor` - we may want a model that can predict the word `replace` is an `activity`, so we would annotate a number of examples texts expressing this information and hopefully be able to teach a model to perform this task automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we commence the following, we'll create a copy of the original data and normalise it with our cleaning function.\n",
    "df_large_cleaned = df_large.copy()\n",
    "\n",
    "# Lets pass the descriptions through our cleaner\n",
    "df_large_cleaned[\"description\"] = df_large_cleaned[\"description\"].apply(\n",
    "    lambda text: text_cleaner(text=text, norm_dict=word_normalisation_dictionary)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.2.C.1 - Basic Keyword Search  <a class=\"anchor\" id=\"3-C-1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The technique we'll explore is `basic keyword searching`, that is, like in spreadsheet software (Excel, etc.) we will search our data for words or phrases that may indicate an end-of-life event. For example, consider the texts:\n",
    "- `replace` pump impeller\n",
    "- Bredel pump `blown`\n",
    "- `cracked` piston in piston pump\n",
    "- PU001 `not pumping` efficiently\n",
    "\n",
    "From these texts, we could elicit the terms `replace`, `blown`, `cracked`, `not pumping` as indicators of end-of-life. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First, lets review our dataset and get some group intuitions for this task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Lets make a copy of our cleaned dataset\n",
    "df_kws = df_large_cleaned.copy()\n",
    "\n",
    "df_kws_len = len(df_kws)  # Get the length of the copied dataset to use for comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Records in dataset: {len(df_kws)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Print dataset\n",
    "df_kws.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Activity 3.C.1 - Identifying end-of-life events (EOL) via key-word search  <a class=\"anchor\" id=\"3-C-1-activity\"></a>\n",
    "\n",
    "Post your answers on [Menti](https://www.menti.com/tkwy8w3mev).\n",
    "\n",
    "With reference to the work order dataset:\n",
    "- What words or phrases would you use to search for end-of-life events?\n",
    "- What limitations do you think there are to the approach of key-word search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lets use the `pandas` package to perform basic keyword search on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is how we could filter all of our records for the term `replace`, similar to what we saw in the previous weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Let's filter our pump dataset for all records that contain the term `replace`\n",
    "# We will ignore the casing of the words here so that replace and Replace are treated equally.\n",
    "\n",
    "search_term = \"replace\"  # Change to any EOL word/phrase\n",
    "\n",
    "df_kws_replace = df_kws[df_kws[\"description\"].str.contains(search_term, case=False)]\n",
    "print(\n",
    "    f\"Number of records containing {search_term}: {len(df_kws_replace)}/{df_kws_len} ({len(df_kws_replace)/df_kws_len*100:0.2f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We obviously know that there are numerous words and phrases that can indicate EOL, so lets expand our search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Lets expand our vocabulary and search for other words\n",
    "# Note: the term `vocabulary` is used here similar to\n",
    "# the dictionary definition, however the subject is specifically words that represent end-of-life events\n",
    "eol_terms = [\"replace\", \"change out\"]\n",
    "pattern = \"|\".join(eol_terms)\n",
    "\n",
    "df_kws_eol = df_kws[df_kws[\"description\"].str.contains(pattern, case=False)]\n",
    "print(\n",
    "    f\"Number of records matching EOL dictionary: {len(df_kws_eol)}/{df_kws_len} ({len(df_kws_eol)/df_kws_len*100:0.2f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.C.1 - Identifying end-of-life events (EOL) via key-word search\n",
    "#### Take aways\n",
    "- Key-word search is useful *IF* your data has no data quality issues *AND* you have an extensive vocabularly of search terms that have broad coverage of end-of-life terms.\n",
    "- Key-word search does not help us distinguish whether the `item` in question is functional. For example, \"replace pump paint\" may include an EOL term \"replace\" but the semantics of the text is not something we would consider as evidence for statistical life data analysis.\n",
    "\n",
    "#### Questions raised\n",
    "- How do we find end-of-life terms to search for, outside of those most common such as `replace`, `change out`, etc?\n",
    "- How do we deal with poor data quality that impacts our search? For example, `rpl` and `replc` being erroneous versions of `replace`.\n",
    "- How do we isolate our searches for functional items that contribute to the lifetime of the asset rather than ancilliaries?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.C.2 - Expanded keyword search using embeddings  <a class=\"anchor\" id=\"3-C-2\"></a>\n",
    "The next technique we will explore uses the concept of word embeddings to create a dictionary (or \"gazetteer\") of search terms that we can use to expand our EOL terms. This is useful if we cannot exhaustively list all of the ways EOL events can be expressed in maintenance texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial steps we'll take here use the concepts we've already been exposed to, namely, ngrams and word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make another copy of our cleaned dataset\n",
    "df_expanded_kws = df_large_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a corpus from our maintenance texts\n",
    "mwo_corpus_expanded_kws = df_expanded_kws[\"description\"].tolist()\n",
    "\n",
    "print(mwo_corpus_expanded_kws[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the assumption that lowercasing the corpus will not impact the ability to identify end-of-life terms as these are not represented as proper nouns, etc. However, removing casing will improve our embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like we did in the fundamentals section, we will lowercase and tokenize all of our texts\n",
    "tokenized_mwo_corpus_expanded_kws = [\n",
    "    word_tokenize(text) for text in mwo_corpus_expanded_kws\n",
    "]\n",
    "\n",
    "print(tokenized_mwo_corpus_expanded_kws[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know that ngrams capture important information, so we'll create a phraser model that automatically identifies ngrams from our corpus\n",
    "phrase_model_expanded_kws = Phrases(\n",
    "    tokenized_mwo_corpus_expanded_kws,\n",
    "    min_count=5,\n",
    "    threshold=10,\n",
    "    connector_words=ENGLISH_CONNECTOR_WORDS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check that our phraser model works as expected\n",
    "phrase_model_expanded_kws[[\"change\", \"out\", \"converyor\", \"idler\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we'll train an embedding model on our phrased data\n",
    "w2v_model_expanded_kws = Word2Vec(\n",
    "    sentences=[phrase_model_expanded_kws[t] for t in tokenized_mwo_corpus_expanded_kws],\n",
    "    vector_size=300,\n",
    "    min_count=5,\n",
    "    window=3,\n",
    "    epochs=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an embedding model trained on our corpus that has been passed through our phrasing model, we can exploit the numerical space of the embedding to find new EOL terms to improve the coverage of our keyword search process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall that given a word, we can find all of those that are similar.\n",
    "# Hence, words used in similar contexts will share similar numerical values (be close in n-dimensional space)\n",
    "# Therefore, we can start with a few seed terms and iteratively explore the embedding vector space to\n",
    "# incrementally build up a vocabularly of EOL terms.\n",
    "\n",
    "# Our initial kws terms (similar to the second part of the previous activity)\n",
    "eol_expanded_kws_terms = [\"replace\", \"change_out\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_terms(\n",
    "    model, term: str, saved_terms: list, irrelevant_terms: list, topn: int\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Function for finding terms similar to a provider term whilst discarding terms already seen.\n",
    "    \"\"\"\n",
    "\n",
    "    tries = 0\n",
    "    max_tries = 100\n",
    "    similar_terms = []\n",
    "    similar_terms_proba = []\n",
    "    while len(similar_terms) <= topn:\n",
    "        similar_terms_temp, similar_terms_proba_temp = zip(\n",
    "            *model.wv.most_similar([term], topn=topn * 10)\n",
    "        )\n",
    "\n",
    "        # remove similar terms already in terms list\n",
    "        new_terms_idx = [\n",
    "            idx\n",
    "            for idx, similar_term in enumerate(similar_terms_temp)\n",
    "            if similar_term not in irrelevant_terms + saved_terms\n",
    "        ]\n",
    "        # slice terms and proba lists\n",
    "        similar_terms_temp_slice = np.array(list(similar_terms_temp))[new_terms_idx]\n",
    "        similar_terms_proba_temp_slice = np.array(list(similar_terms_proba_temp))[\n",
    "            new_terms_idx\n",
    "        ]\n",
    "\n",
    "        similar_terms.extend(similar_terms_temp_slice)\n",
    "        similar_terms_proba.extend(similar_terms_proba_temp_slice)\n",
    "\n",
    "        tries += 1\n",
    "\n",
    "        if tries == max_tries:\n",
    "            return None, None\n",
    "\n",
    "    return (similar_terms[:topn], similar_terms_proba[:topn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you want to see what comes out of this function - uncomment this block of code.\n",
    "# test_terms, test_probs = get_similar_terms(\n",
    "#    model=cleaned_w2v_model,\n",
    "#    term=\"replace\",\n",
    "#    saved_terms=[\"changed\"],\n",
    "#    irrelevant_terms=[\"replaced\", \"change_out\"],\n",
    "#    topn=10,\n",
    "# )\n",
    "# print(test_terms, test_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we can easily identify top-n phrases that are similar to some other given phrase. Now, if we knew some seed end-of-life phrases (such as 'replace', 'change out', etc.), we could easily search the embedding space to find all of the other ones using the semantics and syntactic properties represented in our learnt embedding space. An overview of the process we are going to perform is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/nb_3-1_term_expansion.png\" width=\"75%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application state\n",
    "idx = 0\n",
    "topn = 10\n",
    "terms = [\"replace\", \"change_out\", \"failed\"]\n",
    "irrelevant_terms = []\n",
    "finished = False\n",
    "sampled_terms, _ = get_similar_terms(\n",
    "    model=cleaned_w2v_model,\n",
    "    term=terms[0],\n",
    "    saved_terms=terms,\n",
    "    irrelevant_terms=irrelevant_terms,\n",
    "    topn=topn,\n",
    ")\n",
    "\n",
    "# Application and logic\n",
    "title = widgets.HTML(\n",
    "    value=f\"Term <b>{terms[0]}</b> (1/{len(terms)})\",\n",
    ")\n",
    "\n",
    "button = widgets.Button(\n",
    "    description=\"Save and continue\",\n",
    "    tooltip=\"Click to save and continue to next term\",\n",
    "    button_style=\"\",\n",
    "    icon=\"check\",\n",
    ")\n",
    "\n",
    "finish_btn = widgets.Button(\n",
    "    description=\"Im finished\", tooltip=\"Click to finish\", button_style=\"\", icon=\"check\"\n",
    ")\n",
    "\n",
    "multiselect = widgets.SelectMultiple(\n",
    "    options=sampled_terms,\n",
    "    value=[],\n",
    "    rows=topn,\n",
    ")\n",
    "output = widgets.Output(layout={\"border\": \"1px solid black\"})\n",
    "\n",
    "\n",
    "@output.capture()\n",
    "def on_button_click(b):\n",
    "    global idx, finished, saved_terms, terms, sampled_terms\n",
    "    clear_output()\n",
    "    finished = idx == len(terms) - 1\n",
    "\n",
    "    # Add selected terms to running list\n",
    "    terms.extend(list(multiselect.value))\n",
    "    # Add terms not selected to irrelevant list\n",
    "    irrelevant_terms.extend(list(set(sampled_terms) - set(multiselect.value)))\n",
    "\n",
    "    print(f\"Terms saved: {len(terms)}\\nTerms Discarded: {len(irrelevant_terms)}\")\n",
    "\n",
    "    if not finished:\n",
    "        idx += 1\n",
    "        next_term = terms[idx]\n",
    "        title.value = f\"Term <b>{terms[idx]}</b> ({idx+1}/{len(terms)})\"\n",
    "\n",
    "        # Sample terms\n",
    "        sampled_terms, sampled_probs = get_similar_terms(\n",
    "            model=cleaned_w2v_model,\n",
    "            term=next_term,\n",
    "            saved_terms=terms,\n",
    "            irrelevant_terms=irrelevant_terms,\n",
    "            topn=topn,\n",
    "        )\n",
    "\n",
    "        multiselect.options = sampled_terms\n",
    "\n",
    "    else:\n",
    "        b.disabled = True\n",
    "        multiselect.disabled = True\n",
    "\n",
    "\n",
    "button.on_click(on_button_click)\n",
    "\n",
    "\n",
    "@output.capture()\n",
    "def on_finish_btn_click(b):\n",
    "    global finished\n",
    "    b.disabled = True\n",
    "    multiselect.disabled = True\n",
    "    button.disabled = True\n",
    "\n",
    "\n",
    "finish_btn.on_click(on_finish_btn_click)\n",
    "\n",
    "box_layout = widgets.Layout(\n",
    "    display=\"flex\", flex_flow=\"column\", align_items=\"center\", width=\"50%\"\n",
    ")\n",
    "\n",
    "box = widgets.HBox(\n",
    "    children=[title, multiselect, button, finish_btn, output], layout=box_layout\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to use a widget built using [ipywidgets](https://ipywidgets.readthedocs.io/en/stable/index.html) to interactively identify end-of-life terms (phrases and words) to build up our end-of-life dictionary to improve coverage of our end-of-life identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Show application\n",
    "display(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets save these to our working space (in case we need them at some later point)\n",
    "with open(\"../data/eol_terms.txt\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for term in list(set(terms)):\n",
    "        outfile.write(f\"{term}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've elicited our end-of-life terms and phrases, lets have a look a the terms we identified and those we disregarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the terms we identified\n",
    "list(set(terms))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lets look at the terms we discarded\n",
    "irrelevant_terms[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the coverage we got from using NLP techniques to find additional EOL terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to remove the underscore added by our phrasing model before we can string search as these do not exist in the original texts\n",
    "eol_expanded_kws_terms = [token.replace(\"_\", \" \") for token in list(set(terms))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of expanded search terms: {len(eol_expanded_kws_terms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see, with only a small amount of effort we're able to identify many more records that may have potential for evidence in statistical data life analysis.\n",
    "\n",
    "expanded_pattern = \"|\".join(eol_expanded_kws_terms)\n",
    "\n",
    "df_expanded_kws_eol = df_expanded_kws[\n",
    "    df_expanded_kws[\"description\"].str.contains(expanded_pattern, case=False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"\"\"\n",
    "1. Basic keyword search\n",
    "{len(df_kws_eol)}/{df_kws_len} ({len(df_kws_eol)/df_kws_len*100:0.2f}%)\n",
    "\n",
    "2. Expanded keyword search\n",
    "{len(df_expanded_kws_eol)}/{df_kws_len} ({len(df_expanded_kws_eol)/df_kws_len*100:0.2f}%)\n",
    "\n",
    "Additional records identified\n",
    "{len(df_expanded_kws_eol)-len(df_kws_eol)} ({(1 - (len(df_kws_eol)/len(df_expanded_kws_eol))) * 100:0.1f}%)\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3.C.2 - Expanded keyword search using embeddings  <a class=\"anchor\" id=\"3-C-2-activity\"></a>\n",
    "\n",
    "Post your answers in [Menti](https://www.menti.com/92a4pdpo75)\n",
    "- How many terms/phrases did you find by searching the embedding space?\n",
    "- What are your thoughts on the process we've employed so far?\n",
    "- What limitations do you think there are to this approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.D - Classification of end-of-life events  <a class=\"anchor\" id=\"3-D\"></a>\n",
    "<img src=\"./images/nb_3-1_flow_diagram-D.png\"/>\n",
    "\n",
    "Now that we have an idea of how to identify end-of-life events in maintenance texts, we need to know how to classify these pieces of evidence as either failure or suspension before we can perform Weibull analysis.\n",
    "\n",
    "\n",
    "- End-of-life classification: \n",
    "Now that we have an idea of the process of eliciting end-of-life events from unstructured text in maintenance work orders, how do we classify these into failures or suspensions? \n",
    "\n",
    "- Primer on end-of-life identification and classification: \n",
    "Given a set of MWOs, how do we determine which mention end-of-life events, and of those that do, how do we determine whether they are failures or suspensions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make another copy of the cleaned dataframe\n",
    "df_eol_clf = df_large_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets use the EOL terms we extracted to mark each row in the dataframe as either having an EOL or not\n",
    "# Recall that the `expanded_pattern` was created in the previous section\n",
    "\n",
    "df_eol_clf[\"eol\"] = df_eol_clf[\"description\"].str.contains(expanded_pattern, case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lets look at the boolean column we added with the descriptions and order types\n",
    "df_eol_clf[[\"description\", \"eol\", \"wo_order_type\"]].iloc[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have identified records with potential EOL events, we need to classify them as either failure or suspension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify an identified end-of-life event as failure or suspension, we'll leverage the fortuitous data fields accompanying the work order texts.\n",
    "\n",
    "For our dataset, we'll use the PM codes where we'll use 'expert logic' to say that anything that is PM01/PM03 (corrective/breakdown - if PM03 is applicable) is a failure and anything that is preventative/planned (PM02) is a suspension. If the data provides more granular information, you could use that instead. At the minimum, maintenance work orders should contain PM codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will specify the column we'll use to classify the work as failure/suspension\n",
    "fail_suspension_col = \"wo_order_type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take a look at all the unique values in this column\n",
    "# Like we saw earlier, we can see a clear distribution of the types of work orders that are created.\n",
    "df_eol_clf[fail_suspension_col].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the values in the failure/suspension column, we can perform the classification of the EOL events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will say that anything that is corrective/breakdown is PM01 and\n",
    "# anything that is planned/preventative is PM02. If the order type is anything else (e.g. PM03/PM05) we'll ignore it\n",
    "fail_values = [\"PM01\"]\n",
    "suspension_values = [\"PM02\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets add a new column to our dataset that classifies the record as a failure (F) or suspension (S)\n",
    "df_eol_clf[\"fs_clf\"] = np.where(\n",
    "    (df_eol_clf[fail_suspension_col].isin(suspension_values)),\n",
    "    \"suspension\",\n",
    "    np.where(df_eol_clf[fail_suspension_col].isin(fail_values), \"failure\", \"other\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take a look at the work orders classified as 'other' - do we need to include these?\n",
    "df_eol_clf[df_eol_clf[\"fs_clf\"] == \"other\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both identified potential EOL events and provided a classification for them based on a structured fortuitous field, we can now filter the dataset for records that may be used as evidence for statistical data life analysis. Here, we will remove all records that do not have an EOL event identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, pandas will return the slice of the dataframe for all the instances that are True\n",
    "df_eol_filtered = df_eol_clf[df_eol_clf[\"eol\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Records with potential EOL events: {len(df_eol_filtered)}/{len(df_large)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can move onto using these records in our analysis for reliability measures, we first need to try and reason about whether the events actually manifested. For example, if a work order text indicated the change out of a component but it either hasn't been actioned (has no start date) or has no, or low, associated cost/time, do we believe that this should be included in our analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3.D - Classification of end-of-life events  <a class=\"anchor\" id=\"3-D-activity\"></a>\n",
    "\n",
    "Post your answers to [Menti](https://www.menti.com/edbeiph7ai)\n",
    "- What fields would you use to help reason about whether an end-of-life event likely occurred?\n",
    "- What other data sources might help you reason about end-of-life events?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.2.E - Reasoning about end-of-life events  <a class=\"anchor\" id=\"3-E\"></a>\n",
    "<img src=\"./images/nb_3-1_flow_diagram-E.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dataframe with identified EOL events and classifications, we can further filter the dataframe using intuition. This approach is iterative, but gives us a good rule of thumb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we know that we need the time between event to calculate reliability measures, so lets remove all records that do not have an actual start date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets perform a preliminary filter of the dataframe.\n",
    "# We know to get reliability measures such as MTTF/MTBF we need to know a date, for this work we'll take the actual start date as being the point of end-of-life\n",
    "# However, this date could also be the creation date of the work order or associated notification.\n",
    "df_eol_filtered = df_eol_filtered[~df_eol_filtered[\"actual_start_date\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of filtered records with actual start date {len(df_eol_filtered)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we know that from experience that just because a maintenance work order is raised, it doesn't mean it is executed, so we need a proxy for determining whether work was carried out. Here, we'll use the fortuitious field of actual total cost. However, fields from other linked data sources could also be used, such as spares information to determine whether a part was changed out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets analyse the data we currently have to gauge what threshold seems reasonable to set initially. Note we could use our profiled data at the start of this notebook to gain similar insight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the handly describe function to quickly view the stats on our numerical columns\n",
    "df_eol_filtered.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets set a general threshold for all of the assets at once, however in the future you could improve this logic by finding asset specific thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_threshold = 5000  # dollars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example process for finding mean values for total acutal cost for each floc in our data - We'll leave this as an exercise for those interested to extend this notebook.\n",
    "# df_floc_groups = df_eol_filtered.groupby(by=['functional_loc'])\n",
    "\n",
    "## Here we are going to find the mean value for total actual cost as an initial threshold for our model\n",
    "# floc_thresholds = {}\n",
    "# for name, group in df_floc_groups:\n",
    "#    print(name, group)\n",
    "#\n",
    "#    floc_thresholds[name] = {'total_actual_costs': group['total_actual_costs'].mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eol_filtered = df_eol_filtered[\n",
    "    (cost_threshold <= df_eol_filtered[\"total_actual_costs\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of work orders after filtering operations: {len(df_eol_filtered)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the two expert thresholds on the structured fortuitious data fields, we can move onto converting our classified records into a format suitable for Weibull analysis. However, it should be noted that the thresholds we've blanket applied can be different for each asset. Therefore, eliciting more accurate representations of cost/work requirements for different assets will improve the classification process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3.E - Reasoning about end-of-life events  <a class=\"anchor\" id=\"3-E-activity\"></a>\n",
    "\n",
    "Post your answers in [Menti](https://www.menti.com/41h9wknpgr)\n",
    "- What are the limitations to this approach? How could we make it better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.F - Statistical data life analysis <a class=\"anchor\" id=\"3-F\"></a>\n",
    "\n",
    "<img src=\"./images/nb_3-1_flow_diagram-F.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have filtered our initial dataset down to records that are likely to have had an end-of-life event manifest, we can convert our data into the format expected by the Reliability package for Weibull analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do this, lets first set a threshold for the minimum number of evidence we expect an asset to have before it can have an analysis performed on it. Typically, a minimum of 5 points of evidence are required for statistical significance. By setting it here, we have clear control of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_evidence_points = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets check out how many failures/suspensions we have in our dataset\n",
    "df_eol_filtered[\"fs_clf\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next step will group each of the records by their functional location and create a failure/suspension dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df_eol_filtered.groupby([\"functional_loc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of functional locations at the start of analysis: {len(groups)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will iterate over the groups of functional locations,\n",
    "# skipping past any that do not meet the minimum evidence requirements\n",
    "# Feel free to uncomment some of the print statements to get a better understanding of what is happening\n",
    "\n",
    "fs_data_per_group = {}\n",
    "for name, values in groups:\n",
    "\n",
    "    if len(values) < min_evidence_points:\n",
    "        #         print('N\\t', name)\n",
    "        continue\n",
    "    else:\n",
    "        #         print(len(values))\n",
    "        #         print('Y\\t', name)\n",
    "        values = values[[\"actual_start_date\", \"fs_clf\"]]\n",
    "\n",
    "        #         print(values.head())\n",
    "\n",
    "        # Lets sort the groups data by actual_start_date so that the earliest come first\n",
    "        fs_data = values.sort_values(by=[\"actual_start_date\"], ascending=True)\n",
    "\n",
    "        #         print(fs_data.head())\n",
    "\n",
    "        # Calculate the time between event (we'll use days here)\n",
    "        fs_data[\"time\"] = fs_data[\"actual_start_date\"].diff() / np.timedelta64(1, \"D\")\n",
    "        fs_data[\"time\"] = fs_data[\"time\"].fillna(0)\n",
    "\n",
    "        #         print(fs_data)\n",
    "\n",
    "        # Encode failures and suspensions as 1 and 0, respectively\n",
    "        fs_data[\"fs_clf\"].replace(to_replace=\"failure\", value=1, inplace=True)\n",
    "        fs_data[\"fs_clf\"].replace(to_replace=\"suspension\", value=0, inplace=True)\n",
    "\n",
    "        #         print(fs_data)\n",
    "\n",
    "        fs_data_per_group[name] = fs_data[[\"fs_clf\", \"time\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many flocs do we have sufficient evidence for?\n",
    "print(\n",
    "    f\"Number of functioal locations with sufficient evidence: {len(fs_data_per_group)} / {len(groups)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the groups that have sufficient evidence, we can now use the Reliability package to compute their mean time values using 2-parameter Weibull analysis programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the print statements to get a better understanding of what is happening.\n",
    "\n",
    "results_per_group = {}\n",
    "for name, fs_data in fs_data_per_group.items():\n",
    "\n",
    "    num_evidence = len(fs_data)  #\n",
    "\n",
    "    if num_evidence < min_evidence_points:\n",
    "        pass\n",
    "    else:\n",
    "        fs_data = fs_data[\n",
    "            0 < fs_data[\"time\"]\n",
    "        ]  # Do not take into account any events that have 0 time\n",
    "\n",
    "        failures = fs_data[fs_data[\"fs_clf\"] == 1]\n",
    "        right_censored = fs_data[fs_data[\"fs_clf\"] == 0]  # suspensions\n",
    "\n",
    "        failure_times = failures[\"time\"].tolist()\n",
    "        right_censored_times = right_censored[\"time\"].tolist()\n",
    "\n",
    "        #         print(name)\n",
    "        #         print(failure_times)\n",
    "        #         print(right_censored_times)\n",
    "\n",
    "        if len(set(failure_times)) < 2:\n",
    "            # 2P Weibull expects at least 2 disinct failure points\n",
    "            pass\n",
    "        else:\n",
    "            if 1 < len(failures):\n",
    "                wbfit = Fit_Weibull_2P(\n",
    "                    failures=failure_times,\n",
    "                    right_censored=right_censored_times\n",
    "                    if len(right_censored_times) > 0\n",
    "                    else None,\n",
    "                    show_probability_plot=False,\n",
    "                    print_results=False,\n",
    "                )\n",
    "\n",
    "                mean = wbfit.distribution.mean\n",
    "\n",
    "                if (\n",
    "                    365 * 10 < mean\n",
    "                ):  # Mean greater than 10 years where time unit is days.\n",
    "                    print(\n",
    "                        f\"{name} requires review - very large mean ({mean:0.0f} units)\"\n",
    "                    )\n",
    "                else:\n",
    "                    results_per_group[name] = {\n",
    "                        \"alpha\": wbfit.alpha,\n",
    "                        \"beta\": wbfit.beta,\n",
    "                        \"mean\": mean,\n",
    "                        \"time_on_test\": fs_data[\"time\"].sum(),\n",
    "                        \"evidence\": num_evidence,\n",
    "                    }\n",
    "            else:\n",
    "                print(f\"{name} only has censored events\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets round our values to make them nicer to work with and view what we've found\n",
    "df_results = pd.DataFrame.from_dict(results_per_group).T\n",
    "df_results = df_results.round({\"alpha\": 0, \"beta\": 1, \"mean\": 0})\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets join the object descriptions back onto our data using a `left join`, this will allow us to categorise our data when we're visualising our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we'll pop out the index and make it a column (we'll use this as a key to join on)\n",
    "df_results = df_results.rename_axis(\"functional_loc\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll add the object_desc column to the results dataframe joining on the functional_loc column\n",
    "df_results_with_objs = pd.merge(\n",
    "    df_results,\n",
    "    df_large[[\"functional_loc\", \"object_desc\"]].drop_duplicates(),\n",
    "    on=\"functional_loc\",\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_with_objs.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig_bubble = px.scatter(\n",
    "    df_results_with_objs,\n",
    "    x=\"alpha\",\n",
    "    y=\"beta\",\n",
    "    size=\"evidence\",\n",
    "    color=\"mean\",\n",
    "    symbol=\"object_desc\",\n",
    "    color_continuous_scale=\"Bluered_r\",\n",
    "    custom_data=[\"mean\", \"evidence\", \"object_desc\", \"functional_loc\"],\n",
    ")\n",
    "fig_bubble.update_traces(\n",
    "    hovertemplate=\"<br>\".join(\n",
    "        [\n",
    "            \"Alpha: %{x}\",\n",
    "            \"Beta: %{y}\",\n",
    "            \"Mean: %{customdata[0]}\",\n",
    "            \"Evidence: %{customdata[1]}\",\n",
    "            \"Object: %{customdata[2]}\",\n",
    "            \"FLOC: %{customdata[3]}\",\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_bubble = fig_bubble.update_layout(\n",
    "    title_text=f\"Overview of Results ({len(df_results)} assets)\"\n",
    ")\n",
    "fig_bubble = fig_bubble.update_layout(coloraxis_colorbar=dict(orientation=\"h\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tips: \n",
    "- Double click legend items to isolate them\n",
    "- Drag your mouse cursor to zoom into regions and navigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_bubble.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach we've explored has allowed us to quickly identify, classify and reason about EOL events exhibited in maintenance work order records. Using this information, we have been able to extract reliability measures in a standardised, and scalable, manner. However, the approach we've demonstrated is not a panacea, but rather should be used as a rule of thumb and method for quickly structuring data for this task. In the next notebook, we will dive into this further using interactive visualisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of Data-Driven Reliability Metrics (Part 1)  <a class=\"anchor\" id=\"summary\"></a>\n",
    "\n",
    "We've learnt a lot of new concepts in this notebook such as exploratory data analysis, fundamentals of NLP, how to clean text data, and how to scale our Weibull analysis over thousands of work orders. In the next notebook (*W3-2 Data-Driven Reliability Metrics - Analysis*) we are going to use what we have learnt in this notebook to analyse our results using interactive visualisation techniques similar to what you might find in BI tools. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Appendix  <a class=\"anchor\" id=\"appendix\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**‚ö° Concept - Stemming and lemmatization**</br>\n",
    "Imagine you have the following texts `replaced pump - working well` and `replace pump - working good`. It may be desirable to reduce each word to a root form to reduce the size of the vocabulary within our corpus. Stemming is a technique that stems all words to a common root by removing or replacing word suffixes (e.g. \"replaced\" to \"replace\") whereas lemmatization returns the base form of words (e.g. \"well\" to \"better\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this section we'll need access to an external resource called WordNet\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets instantiate a stemmer from NLTK\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try 'replace pump - not pumping'\n",
    "sentence_to_stem = \"replace pump - not pumping\"\n",
    "sentence_to_stem_tokenized = word_tokenize(sentence_to_stem)\n",
    "\n",
    "# Like the other exercises, we need to tokenize our sentence\n",
    "stemmed_sentence = [stemmer.stem(token) for token in sentence_to_stem_tokenized]\n",
    "print(stemmed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets instantiate a lemmatizer from NLTK\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interchange, interchanger, interchanging\n",
    "\n",
    "sentence_to_lemma = \"interchange interchanger\"\n",
    "sentence_to_lemma_tokenized = word_tokenize(sentence_to_lemma)\n",
    "\n",
    "lemmatized_sentence = [\n",
    "    lemmatizer.lemmatize(token) for token in sentence_to_lemma_tokenized\n",
    "]\n",
    "print(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚ö° Concept - Concordancing**</br>\n",
    "Concordancing gives us a view of every occurrence of a given word, together with some context. For example, we may be interested in the context that the word `seized` occurs in which may result in `...bearing seized on pulley...` and `...lack of lube - seized ...`.\n",
    "\n",
    "Being able to analyse the use of words in context allows us to quicky get an understanding of what is happening in our texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we can perform concordancing, we need to convert our MWO corpus into a single string and then\n",
    "# into a special NLTK text object. We'll need to put a special token into the object to delineate between texts.\n",
    "special_token = \" | \"\n",
    "texts_for_concordancing = Text(word_tokenize(f\"{special_token}\".join(mwo_corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at concordances for single words such as `broken`, or negation such as `not`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_for_concordancing.concordance(\"not\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at concordances between multiple words, such as \"not working\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_for_concordancing.concordance([\"not\", \"working\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the terse nature of maintenance work order short text, concordancing only provides a glimpse into the context that word/phrases are occurring. However, if you have longer documents such as long text or reports, this can provide quick insight."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "2a2ccc53c67f62631b8d9a249097e4dbd32cc773acbcb6419524310ebb9f7f1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
