{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# <center>Technical Information Extraction</center>\n",
    "## <center>Week 4</center>\n",
    "## <center>Interactive Notebook</center>\n",
    "\n",
    "\n",
    "<center>üìö Source: W4 Technical Information Extraction</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è To make the most out of this notebook, some level of Python programming is desirable. If you've never used Python (or any other programming language) checkout the resources [here](https://wiki.python.org/moin/BeginnersGuide/Programmers) to get up to speed. This is not mandatory, but will allow you to modify the code examples and complete the activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è The intention of this notebook is not to teach you how to build complex machine learning algorithms, but instead demonstrate how to go through the process of identifying a problem and using off-the-shelf models on your own data. The overarching goal here is to gain an intuition and practical understanding of supervised machine learning applied to technical texts and how it can support maintenance and reliability engineering workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è The application herein is on maintenance work order records, but the techniques are agnostic and can be applied in any domain and source of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Notebook Overview\n",
    "In the previous two notebooks (W3-1 and W3-1) we focused on eliciting end-of-life terms and phrases to help semi-automate the process of identifying failure and suspension data from maintenance work order records to perform statistical data life analysis at a greater scale. The process we explored leveraged natural language processing (NLP) concepts such as word embeddings to accelerate the process of identifying end-of-life within the short text description of work order records. \n",
    "\n",
    "In this notebook, we are going to continue to NLP but instead of developing dictionaries of terms and phrases representing a specific concept, we'll use a machine learning approach. Specifically, the application of this notebook is to automatically extract information from work order descriptions, where our goal is to aggregate the information from thousands of work orders into knowledge, as exemplified by the figure below.\n",
    "\n",
    "The motiviation for this application is that there exists a wealth of valuable knowledge inside unstructured texts, but gaining insight into them is challenging and arduous due to the vast number created. If we were able to automatically extract information from texts such as work order descriptions, we could focus on answering, or gaining insight into, the following questions:\n",
    "- Which assets have the most activities performed on them, and what activities are they?\n",
    "- What undesirable states are assets participating in?\n",
    "- What behaviour are assets demonstrating?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./images/W4_dik_real_data.png\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Legend**\n",
    "- ‚ö° indicates a new concept\n",
    "- üìå indicates an activity or interactive part of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Week 3 - Recap](#week-3-recap)\n",
    "* [4.1 Demonstration](#4.1-demonstration)\n",
    "    * [Activity 4.1](#4.1-activity)\n",
    "* [4.2 Fundamentals of supervised information extraction from unstructured texts](#4.2-fundamentals-supervised-ie)\n",
    "    * [4.2.1 Supervised Machine Learning (learning from example)](4.2.1-supervised-ml)\n",
    "        * [Activity 4.2.1](#4.2.1-activity)\n",
    "    * [4.2.2 Information Extraction (structuring the unstructured)](#4.2.2-fundamentals-ie)\n",
    "        * [4.2.2.1 Named Entity Recognition](#4.2.2.1-fundamentals-ner)\n",
    "        * [4.2.2.2 Relation Classification](#4.2.2.2-fundamentals-relation-classification)\n",
    "        * [4.2.2.3 Knowledge Graphs (turning information into knowledge)](#4.2.2.3-activity)\n",
    "        * [Activity 4.2.2](#4.2.2-activity)\n",
    "* [4.3 Technical Information Extraction](#4.3-technical-information-extraction)\n",
    "    * [Notebook Setup](#4.3-notebook-setup)\n",
    "    * [4.3.A Process Overview](#4.3.A-process-overview)\n",
    "    * [4.3.B Development of Conceptual Model](#4.3.B-development-of-conceptual-model)\n",
    "        * [Activity 4.3.B](#4.3.B-activity)\n",
    "    * [4.3.C Data Prepartion](#4.3.C-data-preparation)\n",
    "    * [4.3.D Data Curation](#4.3.D-data-curation)\n",
    "        * [Activity 4.3.D](#4.3.D-activity)\n",
    "    * [4.3.E Model Development](#4.3.E-model-development)\n",
    "    * [4.3.F Model Application](#4.3.F-model-application)\n",
    "        * [4.3.F.1 Using a general models on technical data](#4.3.F.1-general-model)\n",
    "        * [Activity 4.3.F](#4.3.F-activity)\n",
    "    * [4.3.G Analysis](#4.3.G-analysis)\n",
    "        * [4.3.G.1 Extracted States](#4.3.G.1-extracted-states)\n",
    "        * [4.3.G.2 Extracted Activities](#4.3.G.2-extracted-activites)\n",
    "        * [4.3.G.3 Extracted Physical Objects](#4.3.G.3-extracted-physical-objects)\n",
    "        * [Activity 4.3.G](#4.3.G-activity)\n",
    "* [4.4 Network Graph Analysis](#4.4-network-graph-analysis)\n",
    "    * [4.4.1 Triple Generation and Network Creation](#4.4.1-triple-generation-network-creation)\n",
    "    * [4.4.2 Network Visualisation and Analysis](#4.4.2-network-visualisation-analysis)\n",
    "        * [4.4.2.1 Functional Location Graph](#4.4.2.1-functional-location-graph)\n",
    "        * [4.4.2.2 Entire Graph](#4.4.2.2-entire-graph)\n",
    "        * [4.4.2.3 Querying the Network Graph](#4.4.2.3-query-network-graph)\n",
    "    * [4.4 Activity](#4.4-activity)\n",
    "* [Summary](#summary)\n",
    "* [Appendix](#appendix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Objectives\n",
    "- Apply a supervised machine learning model to natural language texts in maintenance work order records\n",
    "- Use a pretrained information extraction model to structure unstructured text and perform analysis\n",
    "- Build a simple knowledge graph from maintenance work order records using natural language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Outcomes\n",
    "- Understand the process of developing a conceptualisation of meaning to apply to natural language texts\n",
    "- Understand the process of human-annotation to curate datasets for supervised machine learning\n",
    "- Gain familiarity with training and using supervised machine learning algorithms for natural language processing tasks\n",
    "- Understand the importance of domain experts (like yourselves) in the process of data-driven technology like supervised machine learning\n",
    "- Understand how to use the outputs of deep learning models for gaining insight into maintenance data to support reliability engineering and maintenance decision-making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3 Recap <a class=\"anchor\" id=\"week-3-recap\"></a>\n",
    "\n",
    "Post your answers to [Menti](https://www.menti.com/efoiig7u9u)\n",
    "- Do you have any questions or comments from last week?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 4.1 - Demonstration <a class=\"anchor\" id=\"4.1-demonstration\"></a>\n",
    "Before we get into the details, lets first get a partial idea of what we're trying to accomplish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìå Activity 4.1 <a class=\"anchor\" id=\"4.1-activity\"></a>\n",
    "\n",
    "Provide your answers on [Menti](https://www.menti.com/wsojfffxjp)\n",
    "\n",
    "Given the following maintenance short texts:\n",
    "1. CVR3 roller frame damaged\n",
    "2. CVR 1 replace collapsed idler\n",
    "3. CVR2 impact plate replace / repair\n",
    "4. CVR1 impact plate failed\n",
    "5. CVR4 DRV 1 - change out flexible coupling\n",
    "6. CVR5 replace hard skirts and soft skirts\n",
    "7. CVR6 - return roller collapsed\n",
    "8. CVR7 replace rock jammed idler FR#61\n",
    "9. CVR6 replace spraybar feed pipe elbow\n",
    "10. CVR4 tighten impact frame bolts\n",
    "11. CVR7 soft skirt popped out\n",
    "12. CVR8 adjust left hand side guide rollers on hammocks\n",
    "\n",
    "Identify and count the number of: **activities**, **undesirable states** and **physical objects**.\n",
    "\n",
    "An example of this activity on the text \"replace seized pump impeller\" is:\n",
    "- **activities**: replace (1)\n",
    "- **undesirable states**: seized (1)\n",
    "- **physical objects**: pump, impeller (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing the same activity with a machine learning based NLP model\n",
    "Here we are going to load a machine learning model trained to identify concepts such as `Activity`, `PhysicalObject` and `State` from maintenance work orders, essentially the same task that we performed manually above. We'll ignore the specifics of the code for now and focus on the process that is being performed. But first, lets install a third part package for deep learning based NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have installed the third party package (details to follow), we can import functions from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required functions to download and use the machine learning model\n",
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "\n",
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.abspath(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the documents in Activity 4.1 are put into a small corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a corpus containing the documents from Activity 4.1\n",
    "demo_texts = [\n",
    "\"CVR3 roller frame damaged\",\n",
    "\"CVR 1 replace collapsed idler\",\n",
    "\"CVR2 impact plate replace / repair\",\n",
    "\"CVR1 impact plate failed\",\n",
    "\"CVR4 DRV 1 - change out flexible coupling\",\n",
    "\"CVR5 replace hard skirts and soft skirts\",\n",
    "\"CVR6 - return roller collapsed\",\n",
    "\"CVR7 replace rock jammed idler FR#61\",\n",
    "\"CVR6 replace spraybar feed pipe elbow\",\n",
    "\"CVR4 tighten impact frame bolts\",\n",
    "\"CVR7 soft skirt popped out\",\n",
    "\"CVR8 adjust left hand side guide rollers on hammocks\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our third party package, we'll load a pretrained machine learning model. For now, we'll ignore the specifics, but the high-level intuition here is that we are loading a machine learning model that assigns a category (or label) to the tokens in each document within our corpus. This is similar to the activity we performed manually, e.g. \"replace pump\" $\\rightarrow$ \"replace\" is an activity and \"pump\" is a physical object. This is an important and popular task in NLP called [named entity recognition (NER)](https://en.wikipedia.org/wiki/Named-entity_recognition)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets download and load the pretrained model. Note the download may take a minute or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to trained model\n",
    "url_to_ner_model = \"https://coreskills.blob.core.windows.net/ds-reliability/ner/ds-best-model.pt\"\n",
    "path_to_save_ner_model = os.path.join(dir_path, \"../data/ner/ds-best-model.pt\")\n",
    "\n",
    "if os.path.exists(path_to_save_ner_model):\n",
    "    print('Model already downloaded')\n",
    "else:\n",
    "    print('Downloading model...')\n",
    "    urlretrieve(url_to_ner_model, path_to_save_ner_model)\n",
    "    print('Download finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model that was downloaded\n",
    "demo_model = SequenceTagger.load(\n",
    "    r'../data/ner/ds-best-model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded our machine learning model (how easy was that!), we'll apply it to the documents in our corpus. Again, we'll ignore the details of this, but if you are interested, feel free to examine it closer. What we are doing here is encoding each document in our corpus into a special 'object' and then using the model to make predictions on the tokens. After this, we are extracting the predictions (labels on our tokens) and counting them. We use the magic method `%%time` to record the time taken to perform this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Perform inference on the demonstration texts and output time taken as well as counts\n",
    "demo_sentences = [Sentence(text.lower()) for text in demo_texts]\n",
    "demo_model.predict(demo_sentences)\n",
    "\n",
    "demo_counts = {}\n",
    "for sentence in demo_sentences:\n",
    "    for entity in sentence.get_spans('ner'):\n",
    "        entity_value = entity.get_label(\"ner\").value\n",
    "        if entity_value in demo_counts.keys():\n",
    "            demo_counts[entity_value] = demo_counts[entity_value] + 1\n",
    "        else:\n",
    "            demo_counts[entity_value] = 1\n",
    "    print(f'{sentence}\\n')\n",
    "\n",
    "print(f'\\n{demo_counts}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, in only a few lines of code we were able to make a computer do the same type of task we did manually, except it was extremely quick whilst also being reproducible (e.g. the results would be the same if we ran this 100 times). For routine tasks such as determining what activities are being performed on assets and the behaviour exhibited by assets, using machine learning based NLP can be very useful for extracting information from thousands of work order records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to try out any text that you like\n",
    "single_demo_text = \"replace idler bearing - too hot\"\n",
    "\n",
    "# Encode -> Predict -> Display\n",
    "single_demo_sentence = Sentence(single_demo_text)\n",
    "demo_model.predict(single_demo_sentence)\n",
    "print(single_demo_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Note there are a few erroneous predictions in the demonstration example that are attributed to a number of factors that will be discussed in due course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets remove the model from memory (modern ML models take up a lot of space!)\n",
    "try:\n",
    "    del demo_model\n",
    "except:\n",
    "    print('Model already deleted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this demonstration as motivation, in the following parts of this notebook we are going to:\n",
    "- Review the fundamentals of extracting information from texts using machine learning based NLP, and\n",
    "- Work through the typical process of using machine learning based NLP with a particular emphasis on supervised information extraction (as seen above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Fundamentals of supervised information extraction from unstructured texts <a class=\"anchor\" id=\"4.2-fundamentals-supervised-ie\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can dive into the application of this notebook, like *W3-1*, we need to first get on a similar page about the fundamentals of machine learning based NLP and the specific NLP task of information extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö° 4.2.1 - Supervised Machine Learning (learning from example) <a class=\"anchor\" id=\"4.2.1-supervised-ml\"></a>\n",
    "Supervised learning is one of the most popular forms of machine learning (other forms include unsupervised and reinforcement learning). A semi-formal definition of this category of technique/algorithm is provided below, but in simple terms it is the process of teaching an algorithm to learn from example. Typically, examples are acquired through human elicitation of knowledge usually called **annotation**.\n",
    "\n",
    "> \"Supervised learning, also known as supervised machine learning, is a subcategory of machine learning and artificial intelligence. It is defined by its use of labeled datasets to train algorithms that to classify data or predict outcomes accurately.\" [IBM Cloud Education, 2020](https://www.ibm.com/cloud/learn/supervised-learning)\n",
    "\n",
    "Many of the services that we use day-to-day, in some form another, would have used or benefited from supervised learning. For example, services such as Alexa/Google Home and Google translate leverage supervised machine learning to some extent (services such as Alexa convert your speech to natural language queries).\n",
    "\n",
    "In general, we can characterise supervised machine learning as follows - an algorithm that learns a complex function that maps $x \\rightarrow y$ when provided a set of $(x,y)$ examples. A set of examples could be $(mwo\\_short\\_text, failure\\_or\\_not)$ e.g. `(pump impeller seized, failure)`, `(conveyor belt holed, failure)`, and `(inspect pump bearing, not failure)`, where these examples are provided to a system to learn to classify texts as failures (or not)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"./images/W4_supervised_learning.jpg\" alt=\"supervised learning diagram\" width=\"75%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pairs of examples we supply to the algorithm are typically acquired through human annotation using a conceptual model for the application we want the algorithm to perform. In the example above, our examples were pairs of text and a label of 'failure' or 'not failure'. The label we provided is predefined and constitutes a model of the task we want to perform, in this case, binary classification of text. The algorithm we use to learn this mapping can vary in complexity and difficulty, but the process of $x \\rightarrow y$ is shared regardless of these factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multitudes of supervised learning tasks typically characterised as either classification (e.g. is A the class of X or Y?) or regression tasks (e.g. what is the value of X given A?), but focusing on natural language texts and maintenance/reliability, we could perform the following:\n",
    "- Classify work order records into failure mode codes\n",
    "- Estimate the hours of work or cost required for a work order given short/long text\n",
    "- Classify work order records by information quality\n",
    "- Translate noisy work order texts into clean work order texts\n",
    "- Extract lubrication related information to support cost aggregation\n",
    "- Classify work order on risk level\n",
    "- and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular to natural language and classification, we typically classify texts at two levels:\n",
    "1. document-level (entire document is associated with a label)\n",
    "    - `pump impeller blown` $\\rightarrow$ `failure`\n",
    "2. token-level (each token is associated with a label)\n",
    "    - `pump impeller blown` $\\rightarrow$ `[(pump, PhysicalObject), (impeller, PhysicalObject), (blown, FailureState)]`\n",
    "\n",
    "‚ö†Ô∏è This is an important distinction as token-level classification is the focus of this notebook. The demonstration was a token-level classification task called named entity recognition (NER)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a high-level intuition towards what supervised machine learning is and a general understanding of it with respect to NLP, lets briefly look at the general steps required to make use of this type of technique. The process of supervised learning can be generalised as:\n",
    "1. Development of task description and objectives\n",
    "2. Curation of labelled data\n",
    "3. Development of supervised machine learning algorithm\n",
    "4. Training of supervised machine learning algorithm\n",
    "5. Performance evaluation\n",
    "6. Inference\n",
    "7. Analysis\n",
    "\n",
    "An important take-away when applying supervised machine learning to texts in technical domains such as maintenance and reliability engineering is that the involvement of subject matter experts (SME) is crucial for steps 1, 2, 5, 6 and 7.\n",
    "\n",
    "Why do we care about these details? For those of you that will leverage machine learning based NLP, its useful to have an understanding of the steps required. For those of you that do not plan to perform these tasks, it is valuable to understand the process others may take for you or that have been taken in the services that you may consume in the future (if you aren't already)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./images/W4_ds_timespent.jpg\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìå Activity 4.2.1 <a class=\"anchor\" id=\"4.2.1-activity\"></a>\n",
    "\n",
    "Post your answer to [Menti](https://www.menti.com/kiosdn26mz)\n",
    "- Why do you think that SMEs are crucial for these five steps in the process outlined above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 - ‚ö° Information Extraction (structuring the unstructured) <a class=\"anchor\" id=\"4.2.2-fundamentals-ie\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an intuition towards the two main types of classification tasks in NLP (document and token classification), we'll put our attention on a specific application of NLP called **information extraction**. The reason we care about information extraction is that ~80% of information is unstructured, which is especially true for detailed observations made within maintenance such as short and long text in *maintenance records* and *notifications*, observation sections of *condition monitoring reports* (vibration analysis, lubrication laboratory results), comments/notes in *downtime records*, *work procedures*, and so forth. In maintenance and reliability, our ability to make decisions and understand whether our maintenance strategy is correct is predicated on understanding how our systems are performing. Information extraction allows us to bring structure to the texts people create that describe the way our systems are behaving.\n",
    "\n",
    "Information extraction is broadly two types of NLP tasks:\n",
    "- Named entity recognition, and\n",
    "- Relation extraction or classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4.2.2.1 - ‚ö° Named Entity Recognition <a class=\"anchor\" id=\"4.2.2.1-fundamentals-ner\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A popular way of extracting information from natural language texts is called named entity recognition ([NER](https://en.wikipedia.org/wiki/Named-entity_recognition)). This technique aims to identify and classify spans of tokens within texts. This is what we performed in the first activity and demonstration.\n",
    "\n",
    "Below is an example of named entity recognition (NER) applied to three work order descriptions for a conveyor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"./images/W4_fundamental_ner_markup.png\" width=\"100%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core idea behind NER is to provide structure to unstructured text. The structure that is provided can be arbitrary, but usually is reflective of the domain the text is derived from and what the specific use-case is. In the example above, the structure is elicited by the concepts of `PhysicalObject`, `UndesirableState`, `Replace`, and `FailedState`. However, we could as easily extract `Asset`, `Technician`, `Alarm` from *condition monitoring reports*, `Consumable`, `Degradation` and `Alert` from *FLAC reports*, or `Injury`, `Body Part`, and `Location` from *HSE records*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high-level steps of NER is to:\n",
    "1. Identify `spans` of `tokens` in a given text that might be of interest e.g. `roller frame` in \"CVR3 roller frame damaged\", and\n",
    "2. Assign a predefined `category` of interest that represents the information we desire e.g. `(roller frame)[PhysicalObject]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the notation of `(text)[label]` to denote **entities** that are extracted from texts using NER. Note \"entity\" refers to span of text that is assigned to a label e.g. `(roller frame)[PhysicalObject]` is an entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of NER on the text \"replace engine oil and change out oil filter\":\n",
    "```\n",
    "    replace engine oil and change out oil filter  ->\n",
    "    (replace)[Activity] (engine oil)[PhysicalObject] and (change out)[Activity] (oil filter)[PhysicalObject]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2.2 - ‚ö° Relation Classification <a class=\"anchor\" id=\"4.2.2.2-fundamentals-relation-classification\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to extracting entities from unstructured text, we can also relate the entities to one another to better understand what is happening within the text. Consider the previous example \"replace engine oil and change out oil filter\". Using relations, we could link the `Activities` to the objects/items that they are being performed on, essentially telling us *\"what is being done to whom\"*. Similar to NER, we need to specify the concepts being relationships, such as `has_part` for relating `items` or `PhysicalObjects`, etc. However, we could reasonably extract the following structure from the text \"replace engine oil and change out oil filter\":\n",
    "\n",
    "- `(replace)[Activity]-[hasParticipant]->(engine oil)[PhysicalObject]`\n",
    "- `(change out)[Activity]-[hasParticipant]->(oil filter)[PhysicalObject]`\n",
    "\n",
    "where `(text)[label]-[relationship]->(text)[label]` denotes a relationship between two entities (called a *triplet*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to perform this process over thousands of maintenance texts, we'd be able to understand a lot about the nature of activities and behaviour of our assets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2.3 - ‚ö° Knowledge Graphs (turning information into knowledge) <a class=\"anchor\" id=\"4.2.3-activity\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, by extracting entities using NER and creating relations between them using relation classification/extraction, we end up with things called **triplets** (or triples). These pieces of information can represent **facts** about the domain our texts are created in. These triplets of facts, naturally, can be linked together through shared concepts to create graphs of knowledge aka **knowledge graphs**. Why do we care about graphs? Graphs provide a convenient way to interpret large amounts of disconnected data in a digestable way whilst being able to be queried. Moreover, *Gartner* rank graph technology in the top 10 trending technologies as of 2020-2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./images/W4_data_info_knowledge_insight.jpg\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of the process of converting unstructured text into knowledge can be seen in the diagram below, created from only a handful of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./images/W4_dik_real_data.png\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, imagine you were to perform this process on 100,000 maintenance work orders, with a more detailed schemata of categories for entities and relations. This process would provide rapid, updatable, insight into the performance of assets, activities being performed, failure modes falling outside of maintenance strategies, and so forth. Moreover, the structured information could be combined with other structured fields such as resource and financial information to gain further insight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìå Activity 4.2.2 <a class=\"anchor\" id=\"4.2.2-activity\"></a>\n",
    "\n",
    "- Do you have any questions or comments on what we have gone through?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Technical Information Extraction <a class=\"anchor\" id=\"4.3-technical-information-extraction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Now that we have a concrete intuition towards supervised machine learning and the NLP task of information extraction, we will step through the process of going from unstructured technical text data to knowledge which we will visualise in a network graph. The process we will follow is outined in the figure below, but before we dive in, lets first set up our notebook. -->\n",
    "Before we dive in, lets first set up our notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Setup <a class=\"anchor\" id=\"4.3-notebook-setup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install required third party packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pandas flair torch plotly nb_black tqdm networkx bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install torch --extra-index-url https://download.pytorch.org/whl/cu116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package for ensuring code we write is formatted nicely\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import standard packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "import random\n",
    "import json\n",
    "import math\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import third party packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [pandas](https://github.com/pandas-dev/pandas) - Package for data handling and wrangling\n",
    "- [numpy](https://numpy.org/) - Package for working with numerical arrays.\n",
    "- [tqdm](https://github.com/tqdm/tqdm) - Package for monitoring progress of operations\n",
    "- [flair](https://github.com/flairNLP/flair) - Package for simple state-of-the-art NLP.\n",
    "- [torch](https://pytorch.org/) - Package for machine learning in Python.\n",
    "- [networkx](https://networkx.org/) - Package for working with complex network graphs.\n",
    "- [plotly](https://plotly.com/) - Package for interactive visualisation.\n",
    "- [bokeh](https://docs.bokeh.org/en/latest/) - Package for interactive visualisation.\n",
    "- [panel](https://panel.holoviz.org/reference/panes/Bokeh.html) - Package for turning visualisations into interactive dashboards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Packages for machine learning\n",
    "import flair\n",
    "from flair.data import Corpus, Sentence\n",
    "from flair.datasets import ColumnCorpus\n",
    "from flair.embeddings import FlairEmbeddings, StackedEmbeddings\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n",
    "import torch\n",
    "\n",
    "# Packages for visualisation and network graph\n",
    "import networkx as nx\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import panel as pn\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.models import (\n",
    "    BoxZoomTool,\n",
    "    Circle,\n",
    "    HoverTool,\n",
    "    MultiLine,\n",
    "    Plot,\n",
    "    Range1d,\n",
    "    ResetTool,\n",
    "    ColumnDataSource,\n",
    "    LabelSet,\n",
    "    Legend,\n",
    "    LegendItem,\n",
    "    NodesAndLinkedEdges,\n",
    "    EdgesAndLinkedNodes,\n",
    "    PanTool,\n",
    ")\n",
    "from bokeh.palettes import Spectral4, Category20c\n",
    "from bokeh.plotting import from_networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "pn.extension()\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.A Process Overview <a class=\"anchor\" id=\"4.3.A-process-overview\"></a>\n",
    "\n",
    "</br></br>\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/W4_flow_diagram_overview.png\" width=\"75%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an appreciation for the task of information extraction, what we are going to do is use this technique to try and gain insight into the behaviour of assets and the effectiveness of maintenance strategy using the unstructured text in maintenance work order records. To achieve this, we are going to work through the following steps:\n",
    "1. Develop a conceptual model of information contained within work order texts that will support insight into asset behaviour and maintenance strategy\n",
    "2. Prepare a set of maintenance work order records using what we learnt in Week 3 (*notebook W3-1*)\n",
    "3. Acquire human-annotated training data to use in supervised machine learning\n",
    "4. Develop a supervised machine learning model for information extraction on work order descriptions\n",
    "5. Apply the developed machine learning model to the set of maintenance work order records\n",
    "6. Perform analysis on the data acquired automatically via the machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.B Development of Conceptual Model <a class=\"anchor\" id=\"4.3.B-development-of-conceptual-model\"></a>\n",
    "\n",
    "</br></br>\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/W4_flow_diagram_step_B.png\" width=\"75%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated, our goal / purpose in this notebook is to automatically extract information from natural language descriptions in maintenance work order records as to provide us deeper insight into asset behaviour and the effectiveness of maintenance strategy. Before we can pursue this, we first need to concretely identify and define a conceptual model of information we require to facilitate this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every supervised machine learning task requires some level of conceptual model, where a conceptual model is essentially a set of *concepts* that are used to help people communicate the subject that a model represents. For example, imagine you want to build a document-level classification model that detects whether a work order contains a failure, a conceptual model for this could consist of `failure` and `not failure`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìå Activity 4.3.B <a class=\"anchor\" id=\"4.3.B-activity\"></a>\n",
    "\n",
    "Post your answer to [Menti](https://www.menti.com/euqyg45d2t)\n",
    "- What type of concepts could we use to extract information from maintenance work order descriptions? Can you think of anymore in addition to `Activity`, `Undesirable State` and `Physical Object`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are numerous ways of defining the concepts you use in information extraction, for this notebook we'll stick with `Activity`, `Undesirable State` and `Physical Object`. For each of these, we have additional, more specific, concepts below them such as `Activity/MaintenanceActivity` and `Activity/MaintenanceActivity/Replace`. Having finer detail on the concepts we use allow us to more precisely assign meaning to our unstructured texts, but comes at the cost of difficulty in acquiring examples and training machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated at the start of this notebook, what we're learning is not specific to maintenance texts. Using NER, we could as easily extract `Asset`, `Technician`, `Alarm` from condition monitoring reports, `Consumable`, `Degradation` and `Alert` from FLAC reports, `Injury`, `Body Part`, and `Location` from HSE records, or `Cause` and `Effect` from work order notification long text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the concepts we'll use concrete, they are defined as follows:\n",
    "- `PhysicalObject` - *A thing that physically exists*. Examples include: *gearbox*, *light bulb*, *excavator* \n",
    "- `State` - *A condition that a physical object is in at a specific time*. Examples include: *blown*, *not working*, *snapped*\n",
    "- `Activity` - *A condition in which things are happening or being done to a physical object or state*. Examples include: *overhaul*, *replace*, *diagnose*\n",
    "\n",
    "An example of these concepts identified through information extraction in a work order description looks like:\n",
    "\n",
    "```\n",
    "    replace engine oil and inspect blown hose\n",
    "    \n",
    "    [replace](Activity) [engine oil](PhysicalObject) and [inspect](Activity) [blown](State) [hose](PhysicalObject)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.C Data Preparation <a class=\"anchor\" id=\"4.3.C-data-prepartion\"></a>\n",
    "\n",
    "</br></br>\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/W4_flow_diagram_step_C.png\" width=\"75%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebooks (*W3-1 and W3-2*), we developed a process for preparing and cleaning out natural language texts. Here, we'll use this again on the same dataset we used previously (40,000 conveyor work order records). Unlike notebook W3-2, here we do not need to filter our dataset based on the availability of structured data like `actual_start_date` as we are not computing reliability metrics. However, if you wanted to improve the process of the data-driven reliability metrics by incorporating what we will learn in this notebook, then you would need to use the preparation script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load the script that we created in W3 to clean our work order descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import text_cleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets load and prepare the data for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using your own data, please change the name of the file to the name of your data.\n",
    "# Otherwise, use the URL link provided in this part of the program.\n",
    "path_to_data = \"https://coreskills.blob.core.windows.net/ds-reliability/rh_mod_v1.csv\"  # \"../data/<YOUR_CSV_FILE>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we do not need additional structured fields, but we will see later that this will be useful for performing analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_cols = [\n",
    "    \"id\",\n",
    "    \"description\",\n",
    "    \"wo_order_type\",\n",
    "    \"total_actual_costs\",\n",
    "    \"actual_start_date\",\n",
    "    \"actual_finish_date\",\n",
    "    \"functional_loc_desc\",\n",
    "    \"functional_loc\",\n",
    "]\n",
    "\n",
    "date_cols = [\n",
    "    \"actual_start_date\",\n",
    "    \"actual_finish_date\",\n",
    "]\n",
    "\n",
    "path_to_data = (\n",
    "    path_to_data if \"https\" in path_to_data else os.path.join(dir_path, path_to_data)\n",
    ")\n",
    "\n",
    "df = pd.read_csv(\n",
    "    path_to_data,\n",
    "    parse_dates=date_cols,\n",
    "    dayfirst=True,\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    thousands=\",\",\n",
    "    dtype={\"description\": \"str\", \"total_actual_costs\": \"float\"},\n",
    ")\n",
    "\n",
    "original_df_size = len(df)\n",
    "\n",
    "assert set(expected_cols).issubset(\n",
    "    set(df.columns)\n",
    "), \"Uploaded data does not have all the expected columns\"\n",
    "\n",
    "# Lets remove any descriptions that contain erroneous or missing content (e.g. numbers)\n",
    "df = df[~df[\"description\"].isna()]  # Removes rows that have no description\n",
    "df = df[~df[\"description\"].str.isnumeric()]  # Removes rows that are only numbers\n",
    "\n",
    "# Lets clean the description column using out `text_cleaner` function\n",
    "df[\"description\"] = df[\"description\"].apply(\n",
    "    lambda text: text_cleaner.clean_text(text=text)\n",
    ")\n",
    "\n",
    "# Lets also remove any descriptions that contain a single token as these are unlikely to be useful (you can comment this out if you want to include them)\n",
    "df = df[df[\"description\"].apply(lambda text: 1 < len(text.split(\" \")))]\n",
    "\n",
    "filtered_df_size = len(df)\n",
    "\n",
    "print(f\"Reduced dataframe from {original_df_size} to {filtered_df_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check that our data loading and preprocessing worked as expected. If using the `text_cleaner` script, we should see that the descriptions are lower cased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.D Data Curation <a class=\"anchor\" id=\"4.3.D-data-curation\"></a>\n",
    "\n",
    "</br></br>\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/W4_flow_diagram_step_D.png\" width=\"75%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated in the fundamentals section, we require training examples to teach a supervised machine learning algorithm. Here we'll call the process of acquiring these examples **data curation**. This process is critical in supervised learning and is challenging in technical domains like maintenance and reliability. Unlike in general, everyday, scenarios, the knowledge required to understand content in technical domains is very specific and typically difficult to transfer (it is called [*tacit knowledge*](https://en.wikipedia.org/wiki/Tacit_knowledge)).\n",
    "\n",
    "Consider work orders raised against a bespoke asset that highlight novel failure behaviour that are not readily understandable by lay-people. Instead, it requires specialist knowledge to parse this information and to understand it. Due to this fact, when curating data to teach supervised machine learning algorithms, it is paramount that those that understand the domain, help acquire the data. Hence, subject matter experts are indispensible for many machine learning based tasks that are intended to be used on complex technical information.\n",
    "\n",
    "Unfortunately, many supervised machine learning processes require a large amount of data. To overcome the cost of data acquisition, in general settings, paid human workers can be used through crowd sourcing. That is, people are paid per document to apply labels that are then used to train machine learning models. In technical domains, this is difficult due to tacit knowledge requirements and data cofidentiality. Instead, individuals usually use purpose-built **annotation software** to collect examples for supervised machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The annotation software used typically depends on the task you wish to perform, however for information extraction, the annotation tool **Quickgraph** developed by the [UWA NLP-TLP Group](https://nlp-tlp.org/) can be used to quickly obtain data we require for information extraction. Although there are other options out there including: [Prodigy](https://prodi.gy/) and [LabelStudio](https://labelstud.io/).\n",
    "\n",
    "Unfortunately, due to the time constraints in this part of the program, we cannot go through the process of manually acquiring data. Instead, we'll load some data that has been annotated by a human already. The data we will use acquired over the course of a few hours of effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, you'll have three datasets for supervised machine learning - `training`, `validation` and `test`. Go [here](https://machinelearningmastery.com/difference-test-validation-datasets/) to find out more about what these mean. Lets load the `training` dataset that consists of examples we'll teach our machine learning algorithm with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets load all of the human-annotated datasets and save them to disk\n",
    "data_urls = {\n",
    "    \"train\": \"https://coreskills.blob.core.windows.net/ds-reliability/ner/train.txt\",\n",
    "    \"valid\": \"https://coreskills.blob.core.windows.net/ds-reliability/ner/valid.txt\",\n",
    "    \"test\": \"https://coreskills.blob.core.windows.net/ds-reliability/ner/test.txt\",\n",
    "}\n",
    "\n",
    "for split_name, split_url in data_urls.items():\n",
    "    print(f\"{split_name}: {split_url}\")\n",
    "    path_to_save_data = os.path.join(dir_path, f\"../data/ner/{split_name}.txt\")\n",
    "    urlretrieve(split_url, path_to_save_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets load one of the files that we downloaded (the training dataset)\n",
    "with open(\"../data/ner/train.txt\", \"r\", encoding=\"utf-8\") as infile:\n",
    "    training_data = infile.readlines()\n",
    "    training_data = [line.replace(\"\\n\", \"\") for line in training_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the first few rows of the dataset we have loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in training_data[:20]:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see here is on the left are the tokens in a work order description and on the right are the labels that have been applied to these tokens by a human annotator. The prefixes on the labels are used to indicate whether the tokens are ngrams. For example, two adjacent tokens with `B-PhysicalObject` and `I-PhysicalObject` indicates that the token is a bigram (2-gram). More information about this notation can be found [here](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)) and is specific to token-classification NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of documents we are going to use to train our model\n",
    "training_docs = [\n",
    "    list(group) for _, group in itertools.groupby(training_data, key=\"\".__ne__)\n",
    "]\n",
    "training_docs = [doc for doc in training_docs if doc != [\"\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of training documents: {len(training_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìå Activity 4.3.D <a class=\"anchor\" id=\"4.3.D-activity\"></a>\n",
    "\n",
    "Post your answer to [Menti](https://www.menti.com/6tap8gv81d)\n",
    "- What factors do you think impact the process of acquiring human-labelled data for training machine learning systems?\n",
    "- What do you think the limitations of this small set of data will be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.E Model Development <a class=\"anchor\" id=\"4.3.E-model-development\"></a>\n",
    "\n",
    "</br></br>\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/W4_flow_diagram_step_E.png\" width=\"75%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both an understanding of the concepts we want to use to answer the questions we want to ask on our data and a set of human-annotated examples to train a supervised machine learning model using these concepts, we can train a machine learning model to try and do this process automatically.\n",
    "\n",
    "There are many frameworks/packages that allow the development and traning of machine learning models for NLP in Python such as [Flair](https://github.com/flairNLP/flair), [HuggingFace](https://github.com/huggingface/transformers), [PyTorch](https://github.com/pytorch/pytorch), [FairSeq](https://github.com/facebookresearch/fairseq), [Tensorflow](https://github.com/tensorflow/tensorflow), [Keras](https://github.com/keras-team/keras). In this section, we will use **Flair** as it allows us easy access to state-of-the-art models with low amounts of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we are going to train a named entity recognition model using Flair, but the specific details of the training process is out of the scope of this program. Refer to [this reference](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md) for a more detailed guide to train Flair models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Note the model that we train/use will not be perfect as only a limited amount of data has been acquired for this part of the program. To improve this model, more data is required. If you're interested in extending the model that we are using, feel free to get in contact to get more specific guidance to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ner_model(data_dir: str = \"../data/ner\", model_dir: str = \"../data/ner\"):\n",
    "    \"\"\"Trains a simple named entity recognition model using Flair\"\"\"\n",
    "\n",
    "    assert os.path.isdir(\n",
    "        data_dir\n",
    "    ), \"Directory for data does not exist - please create and add data then try again.\"\n",
    "    assert os.path.isdir(\n",
    "        model_dir\n",
    "    ), \"Directory for model does not exist - please create and try again.\"\n",
    "\n",
    "    flair.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    columns = {0: \"text\", 1: \"ner\"}\n",
    "\n",
    "    # 1. load training data from disk\n",
    "    corpus: Corpus = ColumnCorpus(\n",
    "        data_dir,\n",
    "        columns,\n",
    "        train_file=\"train.txt\",\n",
    "        dev_file=\"valid.txt\",\n",
    "        test_file=\"test.txt\",\n",
    "    )\n",
    "\n",
    "    # 2. specify the type of label we want to predict\n",
    "    label_type = \"ner\"\n",
    "\n",
    "    # 3. make the tag dictionary from the corpus\n",
    "    tag_dictionary = corpus.make_label_dictionary(label_type=label_type)\n",
    "\n",
    "    # 4. specify the type of embeddings we want to use\n",
    "    embeddings: StackedEmbeddings = StackedEmbeddings(\n",
    "        [\n",
    "            FlairEmbeddings(\"mix-forward\"),\n",
    "            FlairEmbeddings(\"mix-backward\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 5. initialize sequence tagger\n",
    "    tagger: SequenceTagger = SequenceTagger(\n",
    "        hidden_size=256,\n",
    "        embeddings=embeddings,\n",
    "        tag_dictionary=tag_dictionary,\n",
    "        tag_type=label_type,\n",
    "        use_crf=True,\n",
    "    )\n",
    "\n",
    "    # 6. initialize trainer\n",
    "    trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "    # 7. start training\n",
    "    trainer.train(\n",
    "        model_dir,\n",
    "        learning_rate=0.1,\n",
    "        mini_batch_size=32,\n",
    "        max_epochs=50,\n",
    "        embeddings_storage_mode=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll either load a model we already have or train one from scratch. Training from scratch will take approximately 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to bypass the training (it may be too slow in the labs environment) we'll use the model we downloaded at the start of the notebook.\n",
    "if os.path.isfile(r\"../data/ner/ds-best-model.pt\"):\n",
    "    ner_model = SequenceTagger.load(r\"../data/ner/ds-best-model.pt\")\n",
    "else:\n",
    "    print(\"Model does not exist - go to the top of the notebook and download it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the code in the two cells below (and comment the cell above) to train the model yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train model from scratch\n",
    "# train_ner_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model (note the name is different compared to the downloaded model (best-model.pt vs ds-best-model.pt)\n",
    "# ner_model = SequenceTagger.load(r'../data/ner/best-model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.F Model Application <a class=\"anchor\" id=\"4.3.F-model-application\"></a>\n",
    "\n",
    "</br></br>\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/W4_flow_diagram_step_F.png\" width=\"75%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have used our human curated training data to train our supervised information extraction model (or loaded the pretrained model), lets try it out on a few texts. This is similar to the process we performed in the demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_texts = [\n",
    "    \"change out leaking mechanical seal\",\n",
    "    \"blown rubber\",\n",
    "    \"replace idler bearing corroded\",\n",
    "    \"conveyor belt holed\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sentence object and make a prediction\n",
    "example_sentences = [Sentence(text.lower()) for text in example_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using our trained model\n",
    "ner_model.predict(example_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out predictions on sentence objects\n",
    "for sentence in example_sentences:\n",
    "    print(f\"{sentence}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.F.1 Using a general models on technical data <a class=\"anchor\" id=\"4.3.F.1-general-model\"></a>\n",
    "Similar to the notebook in Week 3 where we discussed the limitations and caution required when working with general word embeddings, here we'll briefly explore and discuss the use of a general information extraction model on our technical text examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model is trained on a large corpus of general documents with a schema that does not capture the same information above.\n",
    "general_ner_model = SequenceTagger.load(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "example_sentences_general = [Sentence(text.lower()) for text in example_texts]\n",
    "general_ner_model.predict(example_sentences_general)\n",
    "\n",
    "for sentence in example_sentences_general:\n",
    "    print(f\"{sentence}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, using general models on our data without instilling our domain expertise leads to either no results or those that are unexpected. Obviously, this extreme example is contrived and the general model wasn't trained on the same semantics and conceptual model that we used previously, but nonetheless it highlights the need for having specific models for specific problems and domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll delete the general model to free up the memory it took up. Rerun the cells above to load the model again if you like.\n",
    "try:\n",
    "    del general_ner_model\n",
    "except:\n",
    "    print(\"General model already deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìå Activity 4.3.F <a class=\"anchor\" id=\"4.3.F-activity\"></a>\n",
    "\n",
    "Post your answer on [Menti](https://www.menti.com/jnh9u973nk)\n",
    "- What are some observations you've made when changing the example texts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.G Analysis <a class=\"anchor\" id=\"4.3.G-analysis\"></a>\n",
    "\n",
    "</br></br>\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/W4_flow_diagram_step_G.png\" width=\"75%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a common understanding of the fundamentals underpinning machine learning for NLP tasks, we are going to focus our attention on using the model we have created to try and answer the following questions using our work order dataset:\n",
    "1. Which assets have the most activities performed on them, and what activities are they?\n",
    "2. What activities are being performed preventatively versus correctively?\n",
    "3. What behaviour are assets demonstrating?\n",
    "4. Are we seeing failure modes we expect?\n",
    "5. Are we seeing failure modes we didn't expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can attempt to answer these questions, we first need to extract all of the entities from the work order dataset. To summarise the code below, what we are doing is:\n",
    "1. Extracting the work order descriptions from our work order dataframe\n",
    "2. Converting each work order description into a sentence object (expected by our model)\n",
    "3. Making predictions on our sentence objects in batches (to ensure our system doesn't run out of memory or take too long)\n",
    "4. Extracting the predicted entities from the sentence objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(\n",
    "    ner_model: SequenceTagger, df: pd.DataFrame, rows: int = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Extracts entities from maintenance work order record descriptions\"\"\"\n",
    "\n",
    "    if rows != None:\n",
    "        df = df[:rows]\n",
    "\n",
    "    # Convert the dataframe into a list of dictionary objects\n",
    "    mwo_descriptions_with_ids = df[\n",
    "        [\"id\", \"functional_loc\", \"description\", \"wo_order_type\"]\n",
    "    ].to_dict(orient=\"records\")\n",
    "\n",
    "    # We need to build up a list of sentence objects to perform inference on\n",
    "    sentence_objects = [\n",
    "        Sentence(mwo_obj[\"description\"]) for mwo_obj in mwo_descriptions_with_ids\n",
    "    ]\n",
    "\n",
    "    # Perform inference on the list of sentence objects (note: this may take a moment if there are lots of sentences)\n",
    "    batch_size = 64\n",
    "    for i in tqdm(range(0, len(sentence_objects), batch_size), desc=\"Processing texts\"):\n",
    "        ner_model.predict(sentence_objects[i : i + batch_size])\n",
    "\n",
    "    # Add entities to mwo descriptions\n",
    "    for idx, mwo_obj in tqdm(\n",
    "        enumerate(mwo_descriptions_with_ids), desc=\"Extracting entities\"\n",
    "    ):\n",
    "        # sentence_obj should be aligned with idx of mwo_obj\n",
    "        entities = [\n",
    "            (entity.text, entity.get_label(\"ner\").value)\n",
    "            for entity in sentence_objects[idx].get_spans(\"ner\")\n",
    "        ]\n",
    "        mwo_obj[\"entities\"] = entities\n",
    "\n",
    "    mwo_obj_per_group = {}\n",
    "    for mwo_obj in tqdm(mwo_descriptions_with_ids, desc=\"Aggregating entities\"):\n",
    "        floc = mwo_obj[\"functional_loc\"]\n",
    "\n",
    "        if floc in mwo_obj_per_group.keys():\n",
    "            mwo_obj_per_group[floc].append(mwo_obj)\n",
    "        else:\n",
    "            mwo_obj_per_group[floc] = [mwo_obj]\n",
    "\n",
    "    # Process aggregated entities into a format that can be converted into a pandas DataFrame for plotting and analysis.\n",
    "    # We will separate out the dataframes into entities associated with corrective, preventative and any MWO order type.\n",
    "    data_rows = []\n",
    "    data_rows_corrective = []\n",
    "    data_rows_preventative = []\n",
    "    for group in tqdm(mwo_obj_per_group, desc=\"Counting entities\"):\n",
    "        # Corrective entities\n",
    "        corrective_entities = list(\n",
    "            itertools.chain.from_iterable(\n",
    "                [\n",
    "                    item[\"entities\"]\n",
    "                    for item in mwo_obj_per_group[group]\n",
    "                    if item[\"wo_order_type\"] == \"PM01\"\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        corrective_entity_counts = Counter(corrective_entities)\n",
    "        if 0 < len(corrective_entity_counts):\n",
    "            data_rows_corrective.extend(\n",
    "                [\n",
    "                    {\n",
    "                        \"floc\": group,\n",
    "                        \"text\": k[0],\n",
    "                        \"label\": k[1],\n",
    "                        \"freq\": v,\n",
    "                        \"wo_type\": \"corrective\",\n",
    "                    }\n",
    "                    for k, v in corrective_entity_counts.items()\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # Preventative entities\n",
    "        preventative_entities = list(\n",
    "            itertools.chain.from_iterable(\n",
    "                [\n",
    "                    item[\"entities\"]\n",
    "                    for item in mwo_obj_per_group[group]\n",
    "                    if item[\"wo_order_type\"] == \"PM02\"\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        preventative_entity_counts = Counter(preventative_entities)\n",
    "        if 0 < len(preventative_entity_counts):\n",
    "            data_rows_preventative.extend(\n",
    "                [\n",
    "                    {\n",
    "                        \"floc\": group,\n",
    "                        \"text\": k[0],\n",
    "                        \"label\": k[1],\n",
    "                        \"freq\": v,\n",
    "                        \"wo_type\": \"preventative\",\n",
    "                    }\n",
    "                    for k, v in preventative_entity_counts.items()\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # All entities\n",
    "        all_entities = list(\n",
    "            itertools.chain.from_iterable(\n",
    "                [item[\"entities\"] for item in mwo_obj_per_group[group]]\n",
    "            )\n",
    "        )\n",
    "        all_entity_counts = Counter(all_entities)\n",
    "\n",
    "        # Convert to dict objects\n",
    "        data_rows.extend(\n",
    "            [\n",
    "                {\n",
    "                    \"floc\": group,\n",
    "                    \"text\": k[0],\n",
    "                    \"label\": k[1],\n",
    "                    \"freq\": v,\n",
    "                    \"wo_type\": \"all\",\n",
    "                }\n",
    "                for k, v in all_entity_counts.items()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # Create DataFame containing entity information\n",
    "    data_rows_combined = data_rows + data_rows_corrective + data_rows_preventative\n",
    "\n",
    "    # Save the sentence objects with predictions\n",
    "    mwo_objects = [\n",
    "        {**mwo, \"sentence_obj\": sentence_objects[idx]}\n",
    "        for idx, mwo in enumerate(mwo_descriptions_with_ids)\n",
    "    ]\n",
    "\n",
    "    return pd.DataFrame(data_rows_combined), mwo_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets extract all of the entities from the cleaned dataset we loaded earlier in the notebook. Please note that processing all 40,000 documents will take around 15-25 minutes, so by default we'll process a subset of this. However, feel free to run the entire process in your own time to gain further insights. To run the entire dataset, set `rows=None` in the `extract_entities` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Change rows to None if you want to run the entire dataset rows=None\n",
    "df_analysis, mwo_objects = extract_entities(ner_model=ner_model, df=df, rows=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the entities we've extracted from our maintenance work order texts, we can easily perform aggregations on the type of information that our concepts are being applied to automatically. Lets explore the type of entities that have been extracted. Note that we have a column called `wo_type` to help use seperate the aggregated entities for preventative and corrective work order types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check out how big the data we created are\n",
    "print(\n",
    "    f'Total {len(df_analysis)} - Corrective {len(df_analysis[df_analysis[\"wo_type\"] == \"corrective\"])} Preventative {len(df_analysis[df_analysis[\"wo_type\"] == \"preventative\"])}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_analysis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.G.1 Extracted States <a class=\"anchor\" id=\"4.3.G.1-extracted-states\"></a>\n",
    "What type of states have we extracted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_states = (\n",
    "    df_analysis[\n",
    "        (df_analysis[\"wo_type\"] == \"all\") & (df_analysis[\"label\"].str.contains(\"State\"))\n",
    "    ]\n",
    "    .groupby([\"text\", \"label\"])\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "total_states = df_states[\"text\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    df_states,\n",
    "    y=\"text\",\n",
    "    x=\"freq\",\n",
    "    color=\"label\",\n",
    "    title=f\"Distribution of States ({total_states} extracted)\",\n",
    "    orientation=\"h\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    barmode=\"stack\", yaxis={\"categoryorder\": \"total descending\"}, autosize=True\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the states that occur on corrective and preventative work orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "df_states_corr = (\n",
    "    df_analysis[\n",
    "        (df_analysis[\"wo_type\"] == \"corrective\")\n",
    "        & (df_analysis[\"label\"].str.contains(\"State\"))\n",
    "    ]\n",
    "    .groupby([\"text\", \"label\"])\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "df_states_prev = (\n",
    "    df_analysis[\n",
    "        (df_analysis[\"wo_type\"] == \"preventative\")\n",
    "        & (df_analysis[\"label\"].str.contains(\"State\"))\n",
    "    ]\n",
    "    .groupby([\"text\", \"label\"])\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_corr_state = px.bar(\n",
    "    df_states_corr,\n",
    "    y=\"text\",\n",
    "    x=\"freq\",\n",
    "    color=\"label\",\n",
    "    title=f\"Distribution of Extracted Corrective States\",\n",
    "    orientation=\"h\",\n",
    ")\n",
    "fig_corr_state.update_layout(\n",
    "    barmode=\"stack\", yaxis={\"categoryorder\": \"total descending\"}\n",
    ")\n",
    "fig_prev_state = px.bar(\n",
    "    df_states_prev,\n",
    "    y=\"text\",\n",
    "    x=\"freq\",\n",
    "    color=\"label\",\n",
    "    title=f\"Distribution of Extracted Preventative States\",\n",
    "    orientation=\"h\",\n",
    ")\n",
    "fig_prev_state.update_layout(\n",
    "    barmode=\"stack\", yaxis={\"categoryorder\": \"total descending\"}\n",
    ")\n",
    "\n",
    "fig_corr_state.show()\n",
    "fig_prev_state.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.G.2 Extracted Activities <a class=\"anchor\" id=\"4.3.G.2-extracted-activities\"></a>\n",
    "What type of activities have we extracted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_activities = (\n",
    "    df_analysis[\n",
    "        (df_analysis[\"wo_type\"] == \"all\")\n",
    "        & (df_analysis[\"label\"].str.contains(\"Activity\"))\n",
    "    ]\n",
    "    .groupby([\"text\", \"label\"])\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "total_activities = df_activities[\"text\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    df_activities,\n",
    "    y=\"text\",\n",
    "    x=\"freq\",\n",
    "    color=\"label\",\n",
    "    title=f\"Distribution of Activities ({total_activities} extracted)\",\n",
    "    orientation=\"h\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    barmode=\"stack\", yaxis={\"categoryorder\": \"total descending\"}, autosize=True\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the activities performed under corrective and preventative maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_activities_corr = (\n",
    "    df_analysis[\n",
    "        (df_analysis[\"wo_type\"] == \"corrective\")\n",
    "        & (df_analysis[\"label\"].str.contains(\"Activity\"))\n",
    "    ]\n",
    "    .groupby([\"text\", \"label\"])\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "df_activities_prev = (\n",
    "    df_analysis[\n",
    "        (df_analysis[\"wo_type\"] == \"preventative\")\n",
    "        & (df_analysis[\"label\"].str.contains(\"Activity\"))\n",
    "    ]\n",
    "    .groupby([\"text\", \"label\"])\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_corr_act = px.bar(\n",
    "    df_activities_corr,\n",
    "    y=\"text\",\n",
    "    x=\"freq\",\n",
    "    color=\"label\",\n",
    "    title=f\"Distribution of Extracted Corrective Activities\",\n",
    "    orientation=\"h\",\n",
    ")\n",
    "fig_corr_act.update_layout(barmode=\"stack\", yaxis={\"categoryorder\": \"total descending\"})\n",
    "fig_prev_act = px.bar(\n",
    "    df_activities_prev,\n",
    "    y=\"text\",\n",
    "    x=\"freq\",\n",
    "    color=\"label\",\n",
    "    title=f\"Distribution of Extracted Preventative Activities\",\n",
    "    orientation=\"h\",\n",
    ")\n",
    "fig_prev_act.update_layout(barmode=\"stack\", yaxis={\"categoryorder\": \"total descending\"})\n",
    "\n",
    "fig_corr_act.show()\n",
    "fig_prev_act.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.G.3 Extracted Physical Objects <a class=\"anchor\" id=\"4.3.G.3-extracted-physical-objects\"></a>\n",
    "What type of physical objects have we extracted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_physical_objects = (\n",
    "    df_analysis[\n",
    "        (df_analysis[\"wo_type\"] == \"all\")\n",
    "        & (df_analysis[\"label\"].str.contains(\"PhysicalObject\"))\n",
    "    ]\n",
    "    .groupby([\"text\", \"label\"])\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "total_physical_objects = df_physical_objects[\"text\"].nunique()\n",
    "\n",
    "df_physical_objects = df_physical_objects.sort_values(\"freq\", ascending=False)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    df_physical_objects,\n",
    "    y=\"text\",\n",
    "    x=\"freq\",\n",
    "    color=\"label\",\n",
    "    title=f\"Distribution of Top 50 of {total_physical_objects} Physical Objects Extracted\",\n",
    "    orientation=\"h\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    barmode=\"stack\",\n",
    "    yaxis={\"categoryorder\": \"total descending\"},\n",
    "    autosize=True,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.G.4 - Which assets have the most activities performed on them, and what activities are they?\n",
    "Lets take a look at which assets have the most activities performed on them and what activities they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floc_with_most_activities = (\n",
    "    df_analysis[\n",
    "        (df_analysis[\"wo_type\"] == \"all\")\n",
    "        & (df_analysis[\"label\"].str.contains(\"Activity\"))\n",
    "    ]\n",
    "    .groupby(\"floc\")\n",
    "    .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floc_with_most_activities = floc_with_most_activities.sort_values(\n",
    "    by=\"freq\", ascending=False\n",
    ").reset_index()[\"floc\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_floc_max_activities = df_analysis[\n",
    "    (df_analysis[\"wo_type\"] == \"all\")\n",
    "    & (df_analysis[\"floc\"] == floc_with_most_activities)\n",
    "    & (df_analysis[\"label\"].str.contains(\"Activity\"))\n",
    "]\n",
    "fig = px.bar(\n",
    "    df_floc_max_activities,\n",
    "    y=\"text\",\n",
    "    x=\"freq\",\n",
    "    color=\"label\",\n",
    "    title=f\"Group with Most Activities ({floc_with_most_activities})\",\n",
    "    orientation=\"h\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    barmode=\"stack\", yaxis={\"categoryorder\": \"total descending\"}, autosize=True\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"functional_loc\"] == floc_with_most_activities].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see which asset has the most corrective activities performed on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floc_with_most_corr_activities = (\n",
    "    df_analysis[\n",
    "        (df_analysis[\"wo_type\"] == \"corrective\")\n",
    "        & (df_analysis[\"label\"].str.contains(\"Activity\"))\n",
    "    ]\n",
    "    .groupby(\"floc\")\n",
    "    .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floc_with_most_corr_activities = floc_with_most_corr_activities.sort_values(\n",
    "    by=\"freq\", ascending=False\n",
    ").reset_index()[\"floc\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_floc_max_corr_activities = df_analysis[\n",
    "    (df_analysis[\"wo_type\"] == \"corrective\")\n",
    "    & (df_analysis[\"floc\"] == floc_with_most_corr_activities)\n",
    "    & (df_analysis[\"label\"].str.contains(\"Activity\"))\n",
    "]\n",
    "fig_floc_max_corr_act = px.bar(\n",
    "    df_floc_max_corr_activities,\n",
    "    y=\"text\",\n",
    "    x=\"freq\",\n",
    "    color=\"label\",\n",
    "    title=f\"Group with Most Corrective Activities ({floc_with_most_corr_activities})\",\n",
    "    orientation=\"h\",\n",
    ")\n",
    "fig_floc_max_corr_act.update_layout(\n",
    "    barmode=\"stack\", yaxis={\"categoryorder\": \"total descending\"}, autosize=True\n",
    ")\n",
    "fig_floc_max_corr_act.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.G.5 - Which group is exhibiting the most undesirable states?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floc_with_most_undesirable_states = (\n",
    "    df_analysis[\n",
    "        (df_analysis[\"wo_type\"] == \"all\")\n",
    "        & (df_analysis[\"label\"].str.contains(\"UndesirableState\"))\n",
    "    ]\n",
    "    .groupby(\"floc\")\n",
    "    .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floc_with_most_undesirable_states = floc_with_most_undesirable_states.sort_values(\n",
    "    by=\"freq\", ascending=False\n",
    ").reset_index()[\"floc\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_floc_max_activities = df_analysis[\n",
    "    (df_analysis[\"wo_type\"] == \"all\")\n",
    "    & (df_analysis[\"floc\"] == floc_with_most_undesirable_states)\n",
    "    & (df_analysis[\"label\"].str.contains(\"UndesirableState\"))\n",
    "]\n",
    "fig = px.bar(\n",
    "    df_floc_max_activities,\n",
    "    y=\"text\",\n",
    "    x=\"freq\",\n",
    "    color=\"label\",\n",
    "    title=f\"Group with Most Undesirable States ({floc_with_most_undesirable_states})\",\n",
    "    orientation=\"h\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    barmode=\"stack\", yaxis={\"categoryorder\": \"total descending\"}, autosize=True\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"functional_loc\"] == floc_with_most_undesirable_states].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìå Activity 4.3.G <a class=\"anchor\" id=\"4.3.G-activity\"></a>\n",
    "\n",
    "What are your thoughts on what we have done so far? Is there anywhere you can see this being useful in your workflows?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - Network Graph Analysis <a class=\"anchor\" id=\"4.4-network-graph-analysis\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the limitations of what we have done above is that we are unable to discern \"who did what to whom\". The process of eliciting this type of information is complex, so we are going to simplify it down in this section of the notebook. However, the general process we are going to dive into is in the realm of network graph analysis where we will link the entities we have detected in our maintenance work order data with relationships. As mentioned, to keep this simple we will use rules to link entities with the relations `appears_with` and `related_to`. For the intereted individual, more complex automatic extraction of relationships between entities is performed as part of the NLP tasks *relation classification* and *relation extraction*.\n",
    "\n",
    "To make this concrete, consider the work order description \"replace pump impeller\" where we can extract the entities `(replace)[Activity]`, `(pump)[PhysicalObject]` and `(impeller)[PhysicalObject]`. We can add our relations by associating our entities together e.g. `(replace)[Activity]-[appears_with]-(pump)[PhysicalObject]`, `(replace)[Activity]-[appears_with]-(impeller)[PhysicalObject]` and `(pump)[PhysicalObject]-[related_to]-(impeller)[PhysicalObject]`. These items are referred to as **triples** or **triplets**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Unfortunately we are limited to what Python can do for network analysis, but other tools exist that can make this make easier and more insightful, such as all-in-one software like [Neo4J](https://neo4j.com/) or drawing libraries such as [d3](https://d3js.org/).\n",
    "\n",
    "‚ö†Ô∏è A lot of the code used below has been developed for partipants of the program and are required to enable data wrangling and visualisation of the network graphs we are going to create. Feel free to explore these further if you're interested, however the intention here is to provide an expose to this type of analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1 - Triple Generation and Network Creation <a class=\"anchor\" id=\"4.4.1-triple-generation-network-creation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall when we extracted entities from our maintenance work order data we created a large set of `sentence objects` and made predictions over them using our trained machine learning model. Here, we are going to use these objects to generate a set of *triples* that will be used to build our network graph. The triples that we create will be in the form of a list of tuples e.g. `[(subject text, subject label, relation, target text, target label)]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to group all of our information based on the `functional location` but feel free to modify the code to group by other structured fields such as `sort fields` or `functional location descriptions`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create triples from the entities exteacted from each functional location in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_labels = {}\n",
    "group_triples = {}\n",
    "for mwo_object in mwo_objects:\n",
    "    triples = []\n",
    "    floc = mwo_object[\"functional_loc\"]\n",
    "\n",
    "    # Extract the entities from the sentence objects within the mwo_object dictionary\n",
    "    entities = [\n",
    "        (e.text, e.get_label(\"ner\").value)\n",
    "        for e in mwo_object[\"sentence_obj\"].get_spans(\"ner\")\n",
    "    ]\n",
    "\n",
    "    # Create relations using heuristics (real relation extraction/classification is out of scope)\n",
    "    # - PhysicalObject APPEARS_WITH Activity\n",
    "    # - PhysicalObject APPEARS_WITH State\n",
    "    # - PhysicalObject RELATED_TO PhysicalObject (left to right)\n",
    "    phys_obj_entities = [e for e in entities if \"PhysicalObject\" in e[1]]\n",
    "    phys_obj_triples = [\n",
    "        (po_es[0][0], po_es[0][1], \"related_to\", po_es[1][0], po_es[1][1])\n",
    "        for po_es in zip(phys_obj_entities[:-1], phys_obj_entities[1:])\n",
    "    ]\n",
    "    triples.extend(phys_obj_triples)\n",
    "    other_entities = [e for e in entities if \"PhysicalObject\" not in e[1]]\n",
    "\n",
    "    # We are going to make pairwise relations between our entities\n",
    "    for other_entity in other_entities:\n",
    "        other_entity_text, other_entity_label = other_entity\n",
    "        for po_entity in phys_obj_entities:\n",
    "            po_entity_text, po_entity_label = po_entity\n",
    "\n",
    "            # Create triple\n",
    "            triples.append(\n",
    "                (\n",
    "                    other_entity_text,\n",
    "                    other_entity_label,\n",
    "                    \"appears_with\",\n",
    "                    po_entity_text,\n",
    "                    po_entity_label,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    if floc in group_triples.keys():\n",
    "        group_triples[floc].extend(triples)\n",
    "    else:\n",
    "        group_triples[floc] = triples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at the number of triples created per functional location group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get counts of triples on each group\n",
    "group_triple_counts = sorted([len(triples) for triples in group_triples.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_triple_dist = px.bar(\n",
    "    x=range(len(group_triple_counts)), y=group_triple_counts, title=\"Triples per group\"\n",
    ")\n",
    "fig_triple_dist.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define some utility functions for the network we will build below. You can skip these sections, but in general we are defining functions to:\n",
    "- Assign a color to the nodes in our network based on the entity label\n",
    "- Assign a radius to the nodes in our network based on how frequently they occur in the dataset\n",
    "- Aggregate triples together to visualise a large group-agnostic network\n",
    "- Convert extracted triples to a Python NetworkX structure\n",
    "\n",
    "We are using the popular package NetworkX for this analysis, find out more [here](https://networkx.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_color(\n",
    "    label: str, colors={\"activity\": \"blue\", \"state\": \"red\", \"physicalobject\": \"green\"}\n",
    "):\n",
    "    \"\"\"Applies color based on parent name\"\"\"\n",
    "    parent_label = label.lower().split(\"/\")[0]\n",
    "    return colors[parent_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_radius(freq: int, base_radius: int = 10):\n",
    "    \"\"\"Function for using cube root for node radii\"\"\"\n",
    "    return base_radius * (freq ** (1 / 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_triples(group_triples: List[tuple]):\n",
    "    \"\"\"Returns a set of aggregated triples from a set of grouped triples\"\"\"\n",
    "    return list(\n",
    "        itertools.chain.from_iterable([triples for triples in group_triples.values()])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_triples_to_networkx(triples: List[tuple]):\n",
    "    \"\"\"Converts a set of triples into network format (nodes, edges, edge list)\"\"\"\n",
    "\n",
    "    # Get aggregate subgraph from all group triples\n",
    "    all_nodes = list(\n",
    "        itertools.chain.from_iterable(\n",
    "            [[(triple[0], triple[1]), (triple[3], triple[4])] for triple in triples]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Get unique nodes and their frequency (this will be used in the node size/radius)\n",
    "    # Create dict of node value/label tuples and counts\n",
    "    node_counts = Counter(all_nodes)\n",
    "    unique_nodes = list(node_counts.keys())\n",
    "\n",
    "    # Add frequency to unique nodes\n",
    "    unique_node_objects = [\n",
    "        {\n",
    "            \"value\": node[0],\n",
    "            \"label\": node[1],\n",
    "            \"freq\": node_counts[node],\n",
    "            \"radius\": get_node_radius(node_counts[node]),\n",
    "        }\n",
    "        for node in unique_nodes\n",
    "    ]\n",
    "\n",
    "    # Create data required for network\n",
    "    nx_nodes = [\n",
    "        (\n",
    "            node[\"value\"],\n",
    "            {\n",
    "                \"label\": node[\"label\"],\n",
    "                \"color\": get_node_color(node[\"label\"]),\n",
    "                \"value\": node[\"value\"],\n",
    "                \"freq\": node[\"freq\"],\n",
    "                \"radius\": node[\"radius\"],\n",
    "                \"id\": idx,\n",
    "            },\n",
    "        )\n",
    "        for idx, node in enumerate(unique_node_objects)\n",
    "    ]\n",
    "\n",
    "    nx_edges = [(triple[0], triple[3]) for triple in triples]\n",
    "    nx_edge_labels = {(triple[0], triple[3]): triple[2] for triple in triples}\n",
    "\n",
    "    # edge frequencies\n",
    "    edge_counts = Counter(nx_edges)\n",
    "    unique_edges_w_freq = list(edge_counts.keys())\n",
    "\n",
    "    return nx_nodes, nx_edges, nx_edge_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network_graph(nodes, edges, edge_labels):\n",
    "    \"\"\"Creates a network graph data structure using NetworkX\"\"\"\n",
    "\n",
    "    # Create graph\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(nodes)\n",
    "    G.add_edges_from(edges)\n",
    "\n",
    "    # node labels\n",
    "    unique_node_labels = list(\n",
    "        set([(node[1][\"label\"], node[1][\"id\"]) for node in G.nodes(data=True)])\n",
    "    )\n",
    "\n",
    "    # edge classes\n",
    "    unique_edge_labels = list(set([edge for edge in edge_labels.values()]))\n",
    "\n",
    "    return G, edge_labels, unique_node_labels, unique_edge_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_network_graph(\n",
    "    nodes,\n",
    "    edges,\n",
    "    edge_labels,\n",
    "    group_name: str = None,\n",
    "    edge_color_map: dict = {\"related_to\": \"black\", \"appears_with\": \"red\"},\n",
    "):\n",
    "    \"\"\"Creates a network graph using NetworkX and Bokeh/Panel\"\"\"\n",
    "\n",
    "    # Create network graph\n",
    "    G, edge_labels, unique_node_labels, unique_edge_labels = create_network_graph(\n",
    "        nodes, edges, edge_labels\n",
    "    )\n",
    "\n",
    "    # Prepare data for renderers\n",
    "    edge_attrs = {}\n",
    "\n",
    "    for start_node, end_node, _ in G.edges(data=True):\n",
    "        edge_label = edge_labels[(start_node, end_node)]\n",
    "        edge_color = edge_color_map[edge_label]\n",
    "        edge_attrs[(start_node, end_node)] = edge_color\n",
    "\n",
    "    nx.set_edge_attributes(G, edge_attrs, \"edge_color\")\n",
    "\n",
    "    # Show with Bokeh\n",
    "    plot = Plot(\n",
    "        width=800, height=600, x_range=Range1d(-1.5, 1.5), y_range=Range1d(-1.5, 1.5)\n",
    "    )\n",
    "    plot.title.text = (\n",
    "        \"Interactive Maintenance Graph\" + f\" ({group_name})\"\n",
    "        if group_name != None\n",
    "        else \"\"\n",
    "    )\n",
    "\n",
    "    tooltips = None  # [(\"value\", \"@index\"), (\"label\", \"@label\")]\n",
    "    node_hover_tool = HoverTool(tooltips=tooltips)\n",
    "    plot.add_tools(node_hover_tool, BoxZoomTool(), ResetTool(), PanTool())\n",
    "\n",
    "    graph_renderer = from_networkx(G, nx.spring_layout, scale=1, center=(0, 0))\n",
    "\n",
    "    graph_renderer.node_renderer.glyph = Circle(\n",
    "        size=\"radius\", fill_color=\"color\", id=\"id\"\n",
    "    )\n",
    "    graph_renderer.node_renderer.hover_glyph = Circle(size=15, fill_color=Spectral4[1])\n",
    "\n",
    "    graph_renderer.edge_renderer.glyph = MultiLine(\n",
    "        line_alpha=0.5, line_width=1, line_color=\"edge_color\"\n",
    "    )\n",
    "    graph_renderer.edge_renderer.hover_glyph = MultiLine(\n",
    "        line_color=Spectral4[1], line_width=5\n",
    "    )\n",
    "    plot.renderers.append(graph_renderer)\n",
    "\n",
    "    # Add node labels\n",
    "    try:\n",
    "        x, y = zip(*graph_renderer.layout_provider.graph_layout.values())\n",
    "        node_labels = nx.get_node_attributes(G, \"value\")\n",
    "        source = ColumnDataSource(\n",
    "            {\"x\": x, \"y\": y, \"club\": [label for label in node_labels.values()]}\n",
    "        )\n",
    "        labels = LabelSet(x=\"x\", y=\"y\", text=\"club\", source=source)\n",
    "        plot.renderers.append(labels)\n",
    "\n",
    "        # Interative\n",
    "        graph_renderer.selection_policy = NodesAndLinkedEdges()\n",
    "        graph_renderer.inspection_policy = EdgesAndLinkedNodes()\n",
    "\n",
    "        # Add legend\n",
    "        legend_items = [\n",
    "            LegendItem(\n",
    "                label=node_label[0],\n",
    "                renderers=[graph_renderer.node_renderer],\n",
    "                index=node_label[1],\n",
    "            )\n",
    "            for node_label in unique_node_labels\n",
    "        ]\n",
    "        legend = Legend(items=legend_items)\n",
    "        plot.add_layout(legend)\n",
    "        plot.legend.title = \"Nodes\"\n",
    "\n",
    "        # Remove duplicates from legend\n",
    "        legend_tmp = {x.label[\"value\"]: x for x in plot.legend.items}\n",
    "        plot.legend.items.clear()\n",
    "        plot.legend.items.extend(legend_tmp.values())\n",
    "\n",
    "        return show(plot)\n",
    "    except:\n",
    "        print(\"Insufficient data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2 - Network Visualisation and Analysis <a class=\"anchor\" id=\"4.4.2-network-visualisation-analysis\"></a>\n",
    "Now that we have defined the functions required to convert our extracted information into triples to build a network from, we can explore our information and gain insight into the behaviour of our assets more precisely. Note that we are using the relations `appears_with` (<span style=\"color:red;\">red</span>) and `related_to` (black) in our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2.1 Entire Graph <a class=\"anchor\" id=\"4.4.2.1-entire-graph\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also explore the entire graph we've created, but in Python visualising thousands of nodes and edges is difficult for the third party packages we are using. Instead, we'll sample a subset of the entire graph. Feel free to try up the amount of data in your own time outside of the labs environment. You could download the data we have created and visualise it in tools such as Neo4J."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll set the maximum number of triples we want to render in the graph; lower this if any problems arise.\n",
    "MAX_TRIPLES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create object of aggregated triples over all groups\n",
    "all_triples = list(\n",
    "    itertools.chain.from_iterable([triples for triples in group_triples.values()])\n",
    ")\n",
    "print(f\"Number of triples in entire graph = {len(all_triples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lets render the graph (or a subset of it based on our max triples variable)\n",
    "if len(all_triples) < MAX_TRIPLES:\n",
    "    nx_nodes_agg, nx_edges_agg, nx_edge_labels_agg = convert_triples_to_networkx(\n",
    "        triples=all_triples\n",
    "    )\n",
    "    show_network_graph(nx_nodes_agg, nx_edges_agg, nx_edge_labels_agg)\n",
    "else:\n",
    "    print(\n",
    "        f\"Too many triples ({len(all_triples)}) - visualisation will have trouble rendering - reducing graph to max triples\"\n",
    "    )\n",
    "    nx_nodes_agg, nx_edges_agg, nx_edge_labels_agg = convert_triples_to_networkx(\n",
    "        triples=all_triples[:MAX_TRIPLES]\n",
    "    )\n",
    "    show_network_graph(nx_nodes_agg, nx_edges_agg, nx_edge_labels_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2.2 Functional Location Graph <a class=\"anchor\" id=\"4.4.2.1-functional-location-graph\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the larger graph above, it is difficult to precisely see what is happening with a specific asset. Instead of looking at everything, lets examine networks for each functional location group individually. Here we'll randomly sample from the list of functional location groups we processed, but feel free to uncomment the commented code and put any functional location in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show network for a randomly selected group\n",
    "sampled_group = random.choice(list(group_triples.keys()))\n",
    "# sampled_group = '1071-30-05-01-CVR102'\n",
    "\n",
    "assert sampled_group in list(group_triples.keys()), \"Group does not exist - try again.\"\n",
    "\n",
    "nx_nodes, nx_edges, nx_edge_labels = convert_triples_to_networkx(\n",
    "    triples=group_triples[sampled_group]\n",
    ")\n",
    "show_network_graph(\n",
    "    nodes=nx_nodes, edges=nx_edges, edge_labels=nx_edge_labels, group_name=sampled_group\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2.3 Querying the Network Graph <a class=\"anchor\" id=\"4.4.2.3-query-network-graph\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network graphs are useful for visualising extracted information to gain an intuition for what is happening within natural language texts, but we can also query them to show us:\n",
    "- All the states and activities the physical object *X* is involved in, or\n",
    "- All the states the physical object *X* appears with.\n",
    "\n",
    "Note that purpose built graph-based software are better for this type of query work, but NetworkX will suffice for these simple queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets query the graph we have constructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter a term to search the network with\n",
    "search_term = \"brake\"  # Try 'worn', 'brake', 'gearbox', 'fail'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the triples with the search term (we'll limit the triples to the MAX_TRIPLES in case a large graph is returned)\n",
    "search_triples = [\n",
    "    triple\n",
    "    for triple in all_triples\n",
    "    if (search_term in triple[0]) | (search_term in triple[3])\n",
    "]\n",
    "print(f\"Triples containing {search_term} = {len(search_triples)}\")\n",
    "\n",
    "nx_nodes_search, nx_edges_search, nx_edge_labels_search = convert_triples_to_networkx(\n",
    "    triples=search_triples[:MAX_TRIPLES]\n",
    ")\n",
    "show_network_graph(nx_nodes_search, nx_edges_search, nx_edge_labels_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets query the graph to find all problems (undesirable states) with specific physical objects, for example all gearboxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter a physical object search the network with\n",
    "physical_object_search_term = \"gbox\"\n",
    "\n",
    "# Uncomment if you want to randomly sample a physical object to look at\n",
    "# physical_object_search_term = random.choice(\n",
    "#     df_physical_objects[\"text\"].unique().tolist()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the network graph by physical object by name and limit nodes based on their type (note we'll limit the triples based on the MAX_TRIPLES variable)\n",
    "matched_triples = [\n",
    "    triple\n",
    "    for triple in all_triples\n",
    "    if (\n",
    "        (physical_object_search_term in triple[0])\n",
    "        & (\"physicalobject\" in triple[1].lower())\n",
    "        & (\"undesirablestate\" in triple[4].lower())\n",
    "    )\n",
    "    | (\n",
    "        (physical_object_search_term in triple[3])\n",
    "        & (\"physicalobject\" in triple[4].lower())\n",
    "        & (\"undesirablestate\" in triple[1].lower())\n",
    "    )\n",
    "]\n",
    "print(\n",
    "    f'Triples relating to the physical object \"{physical_object_search_term}\" = {len(matched_triples)}'\n",
    ")\n",
    "\n",
    "(\n",
    "    nx_nodes_matched,\n",
    "    nx_edges_matched,\n",
    "    nx_edge_labels_matched,\n",
    ") = convert_triples_to_networkx(triples=matched_triples[:MAX_TRIPLES])\n",
    "show_network_graph(nx_nodes_matched, nx_edges_matched, nx_edge_labels_matched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap Up & Homework <a class=\"anchor\" id=\"summary\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework for next week\n",
    "- Load your own work order datasets into this notebook and rerun\n",
    "- Think about applications of supervised NLP in your own workflows\n",
    "- If you're interested in supervised machine learning - checkout available annotation software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your feedback today is welcome. Provide your answers in [Menti](https://www.menti.com/mqxk64gi8y):\n",
    "- What is one thing you liked about today?\n",
    "- What would you like to see more of?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false,
  "vscode": {
   "interpreter": {
    "hash": "2d1fa75ae63019c3c8a1c7cf36c52696e226563b59559a0664a487ff6aeee3b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
